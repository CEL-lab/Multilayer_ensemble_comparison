{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass (0–3) Substation Classification with Hetero GNN\n",
    "We predict **Classes_4** excluding label `4` (i.e., keep only classes **0–3**). \n",
    "This notebook uses a robust stratified split with per-class minimums, **class-balanced focal loss**, \n",
    "and a **GATv2** hetero model with edge attributes.\n",
    "\n",
    "**Summary from previous runs (this repo):**\n",
    "- Labeled per-class counts: `{2: 45, 3: 77, 1: 13, 0: 59}` (class imbalance).\n",
    "- Best validation macro-F1: **0.5447**; test macro-F1: **0.3554** (on a matching split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# A1 — Setup & Paths\n",
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from torch_geometric.nn import HeteroConv, GATv2Conv\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n",
    "\n",
    "DATA_PATH = \"IncidentDataFinal.csv\"\n",
    "GRAPH_IN  = \"Hetero_Final_NW_graph_fixed_kara.pt\"      # unlabeled graph you already built\n",
    "GRAPH_OUT = \"Hetero_graph_kara_classes4_labeled.pt\"    # will save labeled (0..3 only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incidents: (264458, 31)\n",
      "HeteroData(\n",
      "  substation={\n",
      "    x=[194, 18],\n",
      "    node_ids=[194],\n",
      "    substation_id=[194],\n",
      "  },\n",
      "  (substation, spatial, substation)={\n",
      "    edge_index=[2, 7738],\n",
      "    edge_attr=[7738, 8],\n",
      "  },\n",
      "  (substation, temporal, substation)={\n",
      "    edge_index=[2, 19134],\n",
      "    edge_attr=[19134, 2],\n",
      "  },\n",
      "  (substation, causal, substation)={\n",
      "    edge_index=[2, 7192],\n",
      "    edge_attr=[7192, 4],\n",
      "  }\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.460460.bright04/ipykernel_3041556/3993365680.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  g = torch.load(GRAPH_IN)\n"
     ]
    }
   ],
   "source": [
    "# A2 — Load incidents (CSV) and base graph\n",
    "_cols = pd.read_csv(DATA_PATH, nrows=0).columns\n",
    "_date_cols = [c for c in ['Job OFF Time','Job ON Time'] if c in _cols]\n",
    "inc = pd.read_csv(DATA_PATH, parse_dates=_date_cols)\n",
    "\n",
    "g = torch.load(GRAPH_IN)\n",
    "print(\"Incidents:\", inc.shape)\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled (0..3 only): 194/194 nodes.\n",
      "Saved -> Hetero_graph_kara_classes4_labeled.pt\n"
     ]
    }
   ],
   "source": [
    "# A3 — Attach Classes_4 (drop 4), majority per substation name, save labeled graph\n",
    "def attach_classes4_ignore_4(graph, incident_df, target_col=\"Classes_4\", save_path=None):\n",
    "    g = graph\n",
    "    sub = g['substation']\n",
    "\n",
    "    df = incident_df.copy()\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Missing '{target_col}' in incidents.\")\n",
    "    df['Job Substation'] = df['Job Substation'].astype(str).str.strip().str.upper()\n",
    "    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n",
    "\n",
    "    # keep only labels 0..3\n",
    "    df = df[df[target_col].isin([0,1,2,3])]\n",
    "\n",
    "    labels_by_name = (\n",
    "        df.groupby('Job Substation')[target_col]\n",
    "          .apply(lambda s: s.dropna().astype(int).mode().iloc[0] if s.dropna().size else np.nan)\n",
    "    )\n",
    "\n",
    "    node_names = [str(n).strip().upper() for n in getattr(sub, 'node_ids', [])]\n",
    "    y_list, mask_list = [], []\n",
    "    for name in node_names:\n",
    "        v = labels_by_name.get(name, np.nan)\n",
    "        if pd.isna(v): y_list.append(-1); mask_list.append(False)\n",
    "        else:          y_list.append(int(v)); mask_list.append(True)\n",
    "\n",
    "    sub.y = torch.tensor(y_list, dtype=torch.long)\n",
    "    sub.train_mask = torch.tensor(mask_list, dtype=torch.bool)\n",
    "\n",
    "    labeled = int(sub.train_mask.sum().item())\n",
    "    print(f\"Labeled (0..3 only): {labeled}/{len(node_names)} nodes.\")\n",
    "    if save_path: torch.save(g, save_path); print(\"Saved ->\", save_path)\n",
    "    return g\n",
    "\n",
    "g = attach_classes4_ignore_4(g, inc, \"Classes_4\", save_path=GRAPH_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial edges (undirected): 15476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.460460.bright04/ipykernel_3041556/3120737905.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  g = torch.load(GRAPH_OUT).to(device)\n"
     ]
    }
   ],
   "source": [
    "# A4 — Build tensors, normalize features/edges, make spatial undirected\n",
    "g = torch.load(GRAPH_OUT).to(device)\n",
    "\n",
    "# Edge dicts + per-relation z-score normalization\n",
    "edge_index_dict = {rel: g[rel].edge_index.to(device) for rel in g.edge_types}\n",
    "edge_attr_dict, edge_dim_dict = {}, {}\n",
    "for rel in g.edge_types:\n",
    "    ea = getattr(g[rel], 'edge_attr', None)\n",
    "    if ea is not None and ea.numel() > 0:\n",
    "        m, s = ea.mean(0, keepdim=True), ea.std(0, keepdim=True)\n",
    "        s[s==0] = 1.0\n",
    "        edge_attr_dict[rel] = ((ea - m) / s).to(device)\n",
    "        edge_dim_dict[rel]  = ea.size(1)\n",
    "    else:\n",
    "        edge_attr_dict[rel] = None\n",
    "        edge_dim_dict[rel]  = 0\n",
    "\n",
    "# Node features z-score\n",
    "x_raw = g['substation'].x.to(device)\n",
    "x_mean, x_std = x_raw.mean(0, keepdim=True), x_raw.std(0, keepdim=True)\n",
    "x_std[x_std==0] = 1.0\n",
    "x_dict = {'substation': (x_raw - x_mean) / x_std}\n",
    "\n",
    "# Labels\n",
    "y = g['substation'].y.to(device)\n",
    "num_nodes = x_dict['substation'].size(0)\n",
    "\n",
    "# Make spatial undirected (duplicate reverse edges). Keep temporal/causal directed.\n",
    "if ('substation','spatial','substation') in g.edge_types:\n",
    "    rel = ('substation','spatial','substation')\n",
    "    ei = edge_index_dict[rel]; ea = edge_attr_dict[rel]\n",
    "    rev_ei = torch.stack([ei[1], ei[0]], dim=0)\n",
    "    edge_index_dict[rel] = torch.cat([ei, rev_ei], dim=1)\n",
    "    if ea is not None:\n",
    "        edge_attr_dict[rel] = torch.cat([ea, ea], dim=0)\n",
    "    print(\"Spatial edges (undirected):\", edge_index_dict[rel].size(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled per-class counts: {2: 45, 3: 77, 1: 13, 0: 59}\n",
      "Used split seed: 42\n",
      "train size=131 | class counts: {2: 30, 3: 52, 1: 9, 0: 40}\n",
      "val   size= 31 | class counts: {2: 8, 3: 12, 0: 9, 1: 2}\n",
      "test  size= 32 | class counts: {2: 7, 3: 13, 1: 2, 0: 10}\n"
     ]
    }
   ],
   "source": [
    "# A5 — Robust stratified split with per-class minimums\n",
    "import math\n",
    "\n",
    "def split_counts(mask, name):\n",
    "    yy = y[mask].cpu().numpy()\n",
    "    from collections import Counter\n",
    "    c  = dict(Counter(yy))\n",
    "    print(f\"{name:5} size={int(mask.sum().item()):3d} | class counts:\", c)\n",
    "\n",
    "def _check_feasibility(y_all, labeled_idx, train_frac, val_frac, test_frac, min_per_class):\n",
    "    from collections import Counter\n",
    "    counts = Counter(y_all[labeled_idx])\n",
    "    p_rest = 1.0 - train_frac\n",
    "    p_val  = val_frac / (val_frac + test_frac)\n",
    "    p_test = test_frac / (val_frac + test_frac)\n",
    "    need_val  = {c: math.ceil((min_per_class)/(p_rest*p_val)  + 1e-9) for c in counts}\n",
    "    need_test = {c: math.ceil((min_per_class)/(p_rest*p_test) + 1e-9) for c in counts}\n",
    "    infeasible = {c: (counts[c], max(need_val[c], need_test[c])) for c in counts if counts[c] < max(need_val[c], need_test[c])}\n",
    "    return counts, infeasible\n",
    "\n",
    "def stratified_split_with_min(y_all, labeled_idx, train_frac=0.8, val_frac=0.1, test_frac=0.1,\n",
    "                              min_per_class=2, base_seed=42, max_tries=500):\n",
    "    assert abs(train_frac + val_frac + test_frac - 1.0) < 1e-6\n",
    "    counts, infeasible = _check_feasibility(y_all, labeled_idx, train_frac, val_frac, test_frac, min_per_class)\n",
    "    if infeasible:\n",
    "        msg_lines = [\"Split infeasible with current fractions/min_per_class.\",\n",
    "                     f\"fractions=(train={train_frac:.2f}, val={val_frac:.2f}, test={test_frac:.2f}), min_per_class={min_per_class}\",\n",
    "                     \"Per-class labeled counts:\"]\n",
    "        msg_lines += [f\"  class {c}: n={counts[c]} (needs ≥{need} to ensure val/test ≥{min_per_class})\"\n",
    "                      for c, (_, need) in infeasible.items()]\n",
    "        raise RuntimeError(\"\\n\".join(msg_lines))\n",
    "\n",
    "    y_lab = y_all[labeled_idx]\n",
    "    for t in range(max_tries):\n",
    "        seed = base_seed + t\n",
    "        tr_idx, rest_idx = train_test_split(\n",
    "            labeled_idx, test_size=(1 - train_frac), stratify=y_lab, random_state=seed\n",
    "        )\n",
    "        pos_map = {nid:i for i, nid in enumerate(labeled_idx)}\n",
    "        y_rest  = np.array([y_lab[pos_map[nid]] for nid in rest_idx])\n",
    "\n",
    "        val_idx, test_idx = train_test_split(\n",
    "            rest_idx, test_size=test_frac/(val_frac+test_frac),\n",
    "            stratify=y_rest, random_state=seed\n",
    "        )\n",
    "        from collections import Counter\n",
    "        ok_val  = all(v >= min_per_class for v in Counter(y_all[val_idx]).values())\n",
    "        ok_test = all(v >= min_per_class for v in Counter(y_all[test_idx]).values())\n",
    "        if ok_val and ok_test:\n",
    "            return tr_idx, val_idx, test_idx, seed\n",
    "\n",
    "    raise RuntimeError(\"Could not find a split meeting per-class minimums after reseeding.\")\n",
    "\n",
    "# Build labeled index + labels (0..3 only)\n",
    "labeled_idx  = torch.where(y >= 0)[0].cpu().numpy()\n",
    "y_np         = y.cpu().numpy()\n",
    "\n",
    "# Diagnostics\n",
    "counts, infeasible = _check_feasibility(y_np, labeled_idx, 0.8, 0.1, 0.1, min_per_class=1)\n",
    "print(\"Labeled per-class counts:\", dict(counts))\n",
    "\n",
    "train_idx_l, val_idx_l, test_idx_l, used_seed = stratified_split_with_min(\n",
    "    y_np, labeled_idx, train_frac=0.68, val_frac=0.16, test_frac=0.16, min_per_class=2, base_seed=SEED\n",
    "\n",
    ")\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool, device=device); train_mask[torch.tensor(train_idx_l, device=device)] = True\n",
    "val_mask   = torch.zeros(num_nodes, dtype=torch.bool, device=device); val_mask[torch.tensor(val_idx_l, device=device)]   = True\n",
    "test_mask  = torch.zeros(num_nodes, dtype=torch.bool, device=device); test_mask[torch.tensor(test_idx_l, device=device)]  = True\n",
    "\n",
    "print(\"Used split seed:\", used_seed)\n",
    "split_counts(train_mask, \"train\")\n",
    "split_counts(val_mask,   \"val\")\n",
    "split_counts(test_mask,  \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-balanced alpha: [0.5259618  2.381557   0.6891103  0.40337116] | counts: [59. 13. 45. 77.]\n"
     ]
    }
   ],
   "source": [
    "# B1 — Class-balanced alpha (Cui et al.) + focal CE loss\n",
    "def class_balanced_alpha(labels, num_classes=4, beta=0.9999):\n",
    "    counts = np.bincount(labels, minlength=num_classes).astype(np.float32)\n",
    "    effective_num = 1.0 - np.power(beta, counts)\n",
    "    weights = (1.0 - beta) / np.maximum(effective_num, 1e-8)\n",
    "    weights = weights / weights.sum() * num_classes\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=device), counts\n",
    "\n",
    "ALPHA_CB, cls_counts = class_balanced_alpha(y_np[labeled_idx], num_classes=4, beta=0.9999)\n",
    "print(\"Class-balanced alpha:\", ALPHA_CB.detach().cpu().numpy(), \"| counts:\", cls_counts)\n",
    "\n",
    "def focal_ce_loss(logits, targets, alpha=None, gamma=2.0):\n",
    "    logp = F.log_softmax(logits, dim=1)\n",
    "    p    = torch.exp(logp)\n",
    "    ce   = F.nll_loss(logp, targets, reduction='none')\n",
    "    pt   = p[torch.arange(p.size(0), device=logits.device), targets]\n",
    "    loss = ((1 - pt) ** gamma) * ce\n",
    "    if alpha is not None:\n",
    "        loss = alpha[targets] * loss\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1 — Hetero GATv2 (edge_dim), larger hidden, BN, more dropout\n",
    "class HeteroGATv2Edge(nn.Module):\n",
    "    def __init__(self, hidden=128, out_channels=4, metadata=None, heads=4, dropout=0.35):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        conv1_dict, conv2_dict = {}, {}\n",
    "        for rel in metadata[1]:\n",
    "            edim = edge_dim_dict.get(rel, 0)\n",
    "            if edim > 0:\n",
    "                conv1 = GATv2Conv((-1,-1), hidden, heads=heads, concat=False,\n",
    "                                  edge_dim=edim, add_self_loops=False, dropout=dropout)\n",
    "                conv2 = GATv2Conv((-1,-1), hidden, heads=heads, concat=False,\n",
    "                                  edge_dim=edim, add_self_loops=False, dropout=dropout)\n",
    "            else:\n",
    "                conv1 = GATv2Conv((-1,-1), hidden, heads=heads, concat=False,\n",
    "                                  add_self_loops=False, dropout=dropout)\n",
    "                conv2 = GATv2Conv((-1,-1), hidden, heads=heads, concat=False,\n",
    "                                  add_self_loops=False, dropout=dropout)\n",
    "            conv1_dict[rel] = conv1; conv2_dict[rel] = conv2\n",
    "\n",
    "        self.conv1 = HeteroConv(conv1_dict, aggr='mean')\n",
    "        self.conv2 = HeteroConv(conv2_dict, aggr='mean')\n",
    "        self.bn1   = nn.BatchNorm1d(hidden)\n",
    "        self.bn2   = nn.BatchNorm1d(hidden)\n",
    "        self.lin   = nn.Linear(hidden, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_attr_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict, edge_attr_dict=edge_attr_dict)\n",
    "        x = self.bn1(x_dict['substation'])\n",
    "        x = torch.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x_dict = {'substation': x}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict, edge_attr_dict=edge_attr_dict)\n",
    "        x = self.bn2(x_dict['substation'])\n",
    "        x = torch.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.lin(x)\n",
    "\n",
    "model = HeteroGATv2Edge(metadata=g.metadata()).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0015, weight_decay=2e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', patience=12, factor=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 — RANDOM search (24 trials)\n",
      "\n",
      "=== Trial 1/24 | seed=2024 | cfg={'lr': 0.0007498925553792222, 'wd': 0.00012511985806183071, 'gamma': 1.564455880115518, 'beta': 0.9904161813555952, 'label_smoothing': 0.05, 'clip_norm': 1.0, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0260 | val F1 0.5257 | lr 7.50e-04\n",
      "  Epoch 010 | loss 0.0266 | val F1 0.5000 | lr 5.25e-04\n",
      "  Epoch 020 | loss 0.0115 | val F1 0.5101 | lr 5.25e-04\n",
      "  Epoch 030 | loss 0.0089 | val F1 0.5101 | lr 3.67e-04\n",
      "  Epoch 040 | loss 0.0068 | val F1 0.5311 | lr 2.57e-04\n",
      "\n",
      "=== Trial 2/24 | seed=2025 | cfg={'lr': 9.331389802327092e-05, 'wd': 0.0004608452422319137, 'gamma': 3.7978169196595073, 'beta': 0.9942328975584521, 'label_smoothing': 0.0, 'clip_norm': 0.5, 'plateau_patience': 16, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0047 | val F1 0.5259 | lr 9.33e-05\n",
      "  Epoch 010 | loss 0.0121 | val F1 0.5259 | lr 9.33e-05\n",
      "  Epoch 020 | loss 0.0050 | val F1 0.4804 | lr 4.67e-05\n",
      "\n",
      "=== Trial 3/24 | seed=2026 | cfg={'lr': 0.00013729707156588927, 'wd': 1.733944572453646e-05, 'gamma': 3.9299064344809036, 'beta': 0.9908445407056572, 'label_smoothing': 0.0, 'clip_norm': 1.0, 'plateau_patience': 8, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0063 | val F1 0.5259 | lr 1.37e-04\n",
      "  Epoch 010 | loss 0.0034 | val F1 0.5259 | lr 6.86e-05\n",
      "  Epoch 020 | loss 0.0382 | val F1 0.4721 | lr 3.43e-05\n",
      "\n",
      "=== Trial 4/24 | seed=2027 | cfg={'lr': 0.0020049544994809832, 'wd': 0.000515052184363091, 'gamma': 3.3419997363709406, 'beta': 0.9983122708736005, 'label_smoothing': 0.1, 'clip_norm': 0.5, 'plateau_patience': 12, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0073 | val F1 0.5522 | lr 2.00e-03\n",
      "  Epoch 010 | loss 0.0125 | val F1 0.5141 | lr 2.00e-03\n",
      "  Epoch 020 | loss 0.0063 | val F1 0.5018 | lr 1.00e-03\n",
      "\n",
      "=== Trial 5/24 | seed=2028 | cfg={'lr': 7.38064606306393e-05, 'wd': 3.3516900226163855e-06, 'gamma': 3.044729186263752, 'beta': 0.9924965951819923, 'label_smoothing': 0.05, 'clip_norm': 2.0, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0117 | val F1 0.5259 | lr 7.38e-05\n",
      "  Epoch 010 | loss 0.0037 | val F1 0.5259 | lr 5.17e-05\n",
      "  Epoch 020 | loss 0.0155 | val F1 0.4804 | lr 3.62e-05\n",
      "\n",
      "=== Trial 6/24 | seed=2029 | cfg={'lr': 9.429580038442923e-05, 'wd': 6.194664599969625e-06, 'gamma': 3.6563430908116814, 'beta': 0.9959236085326748, 'label_smoothing': 0.05, 'clip_norm': 2.0, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0275 | val F1 0.5259 | lr 9.43e-05\n",
      "  Epoch 010 | loss 0.0215 | val F1 0.5259 | lr 6.60e-05\n",
      "  Epoch 020 | loss 0.0086 | val F1 0.4804 | lr 4.62e-05\n",
      "\n",
      "=== Trial 7/24 | seed=2030 | cfg={'lr': 0.000263287712361658, 'wd': 0.00010518926874786184, 'gamma': 3.791884122127524, 'beta': 0.9967444244040322, 'label_smoothing': 0.05, 'clip_norm': 1.0, 'plateau_patience': 12, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0089 | val F1 0.5579 | lr 2.63e-04\n",
      "  Epoch 010 | loss 0.0027 | val F1 0.5083 | lr 2.63e-04\n",
      "  Epoch 020 | loss 0.0041 | val F1 0.5083 | lr 1.32e-04\n",
      "\n",
      "=== Trial 8/24 | seed=2031 | cfg={'lr': 0.00028629345761792757, 'wd': 0.00010846602801644763, 'gamma': 1.6183332932737384, 'beta': 0.9955898435319075, 'label_smoothing': 0.0, 'clip_norm': 0.5, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0101 | val F1 0.5259 | lr 2.86e-04\n",
      "  Epoch 010 | loss 0.0086 | val F1 0.5000 | lr 2.00e-04\n",
      "  Epoch 020 | loss 0.0150 | val F1 0.5000 | lr 1.40e-04\n",
      "\n",
      "=== Trial 9/24 | seed=2032 | cfg={'lr': 0.00407500123084298, 'wd': 0.0004516164865977597, 'gamma': 1.8331096875164492, 'beta': 0.9969390347605924, 'label_smoothing': 0.0, 'clip_norm': 2.0, 'plateau_patience': 12, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0257 | val F1 0.4455 | lr 4.08e-03\n",
      "  Epoch 010 | loss 0.0277 | val F1 0.4744 | lr 4.08e-03\n",
      "  Epoch 020 | loss 0.0157 | val F1 0.5053 | lr 4.08e-03\n",
      "  Epoch 030 | loss 0.0184 | val F1 0.4734 | lr 2.04e-03\n",
      "  Epoch 040 | loss 0.0133 | val F1 0.4582 | lr 2.04e-03\n",
      "\n",
      "=== Trial 10/24 | seed=2033 | cfg={'lr': 0.00029127158902271294, 'wd': 0.0008009343337926553, 'gamma': 3.2860476953491604, 'beta': 0.994005550098763, 'label_smoothing': 0.1, 'clip_norm': 2.0, 'plateau_patience': 12, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0426 | val F1 0.5259 | lr 2.91e-04\n",
      "  Epoch 010 | loss 0.0054 | val F1 0.5083 | lr 2.91e-04\n",
      "  Epoch 020 | loss 0.0085 | val F1 0.5086 | lr 2.04e-04\n",
      "\n",
      "=== Trial 11/24 | seed=2034 | cfg={'lr': 7.201206301825496e-05, 'wd': 0.00045641055970632293, 'gamma': 3.9607504737470833, 'beta': 0.9941435706170872, 'label_smoothing': 0.0, 'clip_norm': 0.5, 'plateau_patience': 8, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0019 | val F1 0.5579 | lr 7.20e-05\n",
      "  Epoch 010 | loss 0.0077 | val F1 0.5259 | lr 3.60e-05\n",
      "  Epoch 020 | loss 0.0034 | val F1 0.4804 | lr 1.80e-05\n",
      "\n",
      "=== Trial 12/24 | seed=2035 | cfg={'lr': 0.00010170677312964111, 'wd': 0.0002977256390865494, 'gamma': 2.4808516363335267, 'beta': 0.9990865004861733, 'label_smoothing': 0.0, 'clip_norm': 1.0, 'plateau_patience': 16, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0126 | val F1 0.5259 | lr 1.02e-04\n",
      "  Epoch 010 | loss 0.0076 | val F1 0.5083 | lr 1.02e-04\n",
      "  Epoch 020 | loss 0.0072 | val F1 0.5083 | lr 5.09e-05\n",
      "\n",
      "=== Trial 13/24 | seed=2036 | cfg={'lr': 0.002453839754773073, 'wd': 4.413805253117144e-06, 'gamma': 1.1022586506694552, 'beta': 0.9922789815426686, 'label_smoothing': 0.0, 'clip_norm': 2.0, 'plateau_patience': 12, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0271 | val F1 0.4260 | lr 2.45e-03\n",
      "  Epoch 010 | loss 0.0376 | val F1 0.5290 | lr 2.45e-03\n",
      "  Epoch 020 | loss 0.0201 | val F1 0.4643 | lr 2.45e-03\n",
      "  Epoch 030 | loss 0.0075 | val F1 0.5276 | lr 1.72e-03\n",
      "\n",
      "=== Trial 14/24 | seed=2037 | cfg={'lr': 0.0007571542369809336, 'wd': 3.651081981415952e-05, 'gamma': 3.117814148568785, 'beta': 0.9950187292885274, 'label_smoothing': 0.1, 'clip_norm': 1.0, 'plateau_patience': 12, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0030 | val F1 0.5579 | lr 7.57e-04\n",
      "  Epoch 010 | loss 0.0111 | val F1 0.4736 | lr 7.57e-04\n",
      "  Epoch 020 | loss 0.0040 | val F1 0.5319 | lr 3.79e-04\n",
      "\n",
      "=== Trial 15/24 | seed=2038 | cfg={'lr': 0.003612577673788591, 'wd': 4.652395805016063e-06, 'gamma': 2.4052928945009735, 'beta': 0.9966590036751164, 'label_smoothing': 0.05, 'clip_norm': 1.0, 'plateau_patience': 16, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0287 | val F1 0.4622 | lr 3.61e-03\n",
      "  Epoch 010 | loss 0.0403 | val F1 0.4041 | lr 3.61e-03\n",
      "  Epoch 020 | loss 0.0089 | val F1 0.5174 | lr 3.61e-03\n",
      "  Epoch 030 | loss 0.0042 | val F1 0.5150 | lr 3.61e-03\n",
      "  Epoch 040 | loss 0.0158 | val F1 0.4957 | lr 3.61e-03\n",
      "  Epoch 050 | loss 0.0153 | val F1 0.4895 | lr 3.61e-03\n",
      "  Epoch 060 | loss 0.0105 | val F1 0.4954 | lr 2.53e-03\n",
      "\n",
      "=== Trial 16/24 | seed=2039 | cfg={'lr': 0.0018961777387288589, 'wd': 0.0006568840941709043, 'gamma': 1.9010228325536214, 'beta': 0.9940621979542147, 'label_smoothing': 0.1, 'clip_norm': 2.0, 'plateau_patience': 8, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0551 | val F1 0.4734 | lr 1.90e-03\n",
      "  Epoch 010 | loss 0.0438 | val F1 0.5101 | lr 1.90e-03\n",
      "  Epoch 020 | loss 0.0345 | val F1 0.4683 | lr 9.48e-04\n",
      "  Epoch 030 | loss 0.0157 | val F1 0.4660 | lr 4.74e-04\n",
      "\n",
      "=== Trial 17/24 | seed=2040 | cfg={'lr': 0.004678186464332436, 'wd': 3.14280901334886e-06, 'gamma': 3.342487225772438, 'beta': 0.9905963698507055, 'label_smoothing': 0.1, 'clip_norm': 2.0, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0153 | val F1 0.5308 | lr 4.68e-03\n",
      "  Epoch 010 | loss 0.0487 | val F1 0.4873 | lr 4.68e-03\n",
      "  Epoch 020 | loss 0.0140 | val F1 0.4895 | lr 4.68e-03\n",
      "  Epoch 030 | loss 0.0162 | val F1 0.4674 | lr 3.27e-03\n",
      "\n",
      "=== Trial 18/24 | seed=2041 | cfg={'lr': 0.003781494566263657, 'wd': 1.0491654602957282e-05, 'gamma': 3.4737368696346302, 'beta': 0.9943680298651818, 'label_smoothing': 0.1, 'clip_norm': 0.5, 'plateau_patience': 16, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0095 | val F1 0.4848 | lr 3.78e-03\n",
      "  Epoch 010 | loss 0.0324 | val F1 0.4903 | lr 3.78e-03\n",
      "  Epoch 020 | loss 0.0306 | val F1 0.4004 | lr 3.78e-03\n",
      "  Epoch 030 | loss 0.0109 | val F1 0.4904 | lr 2.65e-03\n",
      "  Epoch 040 | loss 0.0359 | val F1 0.4732 | lr 2.65e-03\n",
      "  Epoch 050 | loss 0.0138 | val F1 0.4800 | lr 1.85e-03\n",
      "\n",
      "=== Trial 19/24 | seed=2042 | cfg={'lr': 0.0018123759707171648, 'wd': 7.300007152798481e-05, 'gamma': 2.605461607641975, 'beta': 0.9958619640148105, 'label_smoothing': 0.0, 'clip_norm': 1.0, 'plateau_patience': 8, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0306 | val F1 0.5311 | lr 1.81e-03\n",
      "  Epoch 010 | loss 0.0186 | val F1 0.4474 | lr 9.06e-04\n",
      "  Epoch 020 | loss 0.0061 | val F1 0.5110 | lr 4.53e-04\n",
      "\n",
      "=== Trial 20/24 | seed=2043 | cfg={'lr': 5.1304398547109135e-05, 'wd': 5.98318210372446e-06, 'gamma': 1.4294866632927246, 'beta': 0.9916921882510137, 'label_smoothing': 0.0, 'clip_norm': 0.5, 'plateau_patience': 16, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0239 | val F1 0.5579 | lr 5.13e-05\n",
      "  Epoch 010 | loss 0.0164 | val F1 0.5000 | lr 5.13e-05\n",
      "  Epoch 020 | loss 0.0321 | val F1 0.4804 | lr 2.57e-05\n",
      "\n",
      "=== Trial 21/24 | seed=2044 | cfg={'lr': 0.000313006886930138, 'wd': 5.734392767608841e-06, 'gamma': 2.8190608141359657, 'beta': 0.9947044501593215, 'label_smoothing': 0.1, 'clip_norm': 0.5, 'plateau_patience': 12, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0160 | val F1 0.5259 | lr 3.13e-04\n",
      "  Epoch 010 | loss 0.0073 | val F1 0.5000 | lr 3.13e-04\n",
      "  Epoch 020 | loss 0.0091 | val F1 0.5000 | lr 1.57e-04\n",
      "\n",
      "=== Trial 22/24 | seed=2045 | cfg={'lr': 0.0024395656652841623, 'wd': 0.0004536143865973778, 'gamma': 1.4184706679413193, 'beta': 0.9905751551489399, 'label_smoothing': 0.0, 'clip_norm': 2.0, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0560 | val F1 0.4753 | lr 2.44e-03\n",
      "  Epoch 010 | loss 0.0101 | val F1 0.4889 | lr 2.44e-03\n",
      "  Epoch 020 | loss 0.0373 | val F1 0.4833 | lr 2.44e-03\n",
      "  Epoch 030 | loss 0.0130 | val F1 0.5175 | lr 1.71e-03\n",
      "  Epoch 040 | loss 0.0103 | val F1 0.5342 | lr 1.20e-03\n",
      "  Epoch 050 | loss 0.0068 | val F1 0.5269 | lr 8.37e-04\n",
      "  Epoch 060 | loss 0.0122 | val F1 0.4967 | lr 5.86e-04\n",
      "\n",
      "=== Trial 23/24 | seed=2046 | cfg={'lr': 0.0030828634274484095, 'wd': 6.140948639230035e-05, 'gamma': 2.3066912542243276, 'beta': 0.9914541937198337, 'label_smoothing': 0.1, 'clip_norm': 2.0, 'plateau_patience': 16, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0340 | val F1 0.4736 | lr 3.08e-03\n",
      "  Epoch 010 | loss 0.0272 | val F1 0.4702 | lr 3.08e-03\n",
      "  Epoch 020 | loss 0.0396 | val F1 0.5078 | lr 3.08e-03\n",
      "\n",
      "=== Trial 24/24 | seed=2047 | cfg={'lr': 0.0014976301463030767, 'wd': 0.0005128483196146302, 'gamma': 2.678057177070774, 'beta': 0.9903708647261621, 'label_smoothing': 0.05, 'clip_norm': 1.0, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0190 | val F1 0.5033 | lr 1.50e-03\n",
      "  Epoch 010 | loss 0.0076 | val F1 0.5021 | lr 1.50e-03\n",
      "  Epoch 020 | loss 0.0039 | val F1 0.4826 | lr 1.05e-03\n",
      "  Epoch 030 | loss 0.0150 | val F1 0.5110 | lr 7.34e-04\n",
      "  Epoch 040 | loss 0.0042 | val F1 0.4873 | lr 5.14e-04\n",
      "\n",
      "Top-5 after Stage 1 (by val F1):\n",
      "  valF1=0.6152 | testF1=0.4579 | epoch=013 | lr=4.68e-03, wd=3.1e-06, γ=3.34, β=0.9906, ls=0.1, clip=2.0\n",
      "  valF1=0.5628 | testF1=0.4857 | epoch=004 | lr=3.08e-03, wd=6.1e-05, γ=2.31, β=0.9915, ls=0.1, clip=2.0\n",
      "  valF1=0.5579 | testF1=0.4728 | epoch=001 | lr=7.57e-04, wd=3.7e-05, γ=3.12, β=0.9950, ls=0.1, clip=1.0\n",
      "  valF1=0.5579 | testF1=0.4338 | epoch=001 | lr=2.63e-04, wd=1.1e-04, γ=3.79, β=0.9967, ls=0.05, clip=1.0\n",
      "  valF1=0.5579 | testF1=0.4338 | epoch=001 | lr=7.20e-05, wd=4.6e-04, γ=3.96, β=0.9941, ls=0.0, clip=0.5\n",
      "\n",
      "Stage 2 — LOCAL refinement around best (± jitter) with 12 trials\n",
      "\n",
      "=== Local Trial 1/12 | seed=12024 | cfg={'lr': 0.003637640522497473, 'wd': 4.572220731241563e-06, 'gamma': 3.5686597550460366, 'beta': 0.9904939886286429, 'label_smoothing': 0.1, 'clip_norm': 2.0, 'plateau_patience': 16, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0087 | val F1 0.3868 | lr 3.64e-03\n",
      "  Epoch 010 | loss 0.0233 | val F1 0.4664 | lr 3.64e-03\n",
      "  Epoch 020 | loss 0.0141 | val F1 0.4947 | lr 3.64e-03\n",
      "  Epoch 030 | loss 0.0117 | val F1 0.4583 | lr 2.55e-03\n",
      "  Epoch 040 | loss 0.0253 | val F1 0.4676 | lr 1.78e-03\n",
      "\n",
      "=== Local Trial 2/12 | seed=12025 | cfg={'lr': 0.005, 'wd': 4.402686296492324e-06, 'gamma': 2.833136752331304, 'beta': 0.9906498172272457, 'label_smoothing': 0.1, 'clip_norm': 0.5, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0106 | val F1 0.3746 | lr 5.00e-03\n",
      "  Epoch 010 | loss 0.0437 | val F1 0.4391 | lr 5.00e-03\n",
      "  Epoch 020 | loss 0.0280 | val F1 0.5579 | lr 3.50e-03\n",
      "  Epoch 030 | loss 0.0112 | val F1 0.5512 | lr 2.45e-03\n",
      "  Epoch 040 | loss 0.0059 | val F1 0.5405 | lr 2.45e-03\n",
      "  Epoch 050 | loss 0.0057 | val F1 0.5158 | lr 1.20e-03\n",
      "\n",
      "=== Local Trial 3/12 | seed=12026 | cfg={'lr': 0.004504803930528735, 'wd': 3.728837938954097e-06, 'gamma': 3.161666061071303, 'beta': 0.9908056318256301, 'label_smoothing': 0.05, 'clip_norm': 2.0, 'plateau_patience': 16, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0051 | val F1 0.4736 | lr 4.50e-03\n",
      "  Epoch 010 | loss 0.0599 | val F1 0.4800 | lr 4.50e-03\n",
      "  Epoch 020 | loss 0.0218 | val F1 0.4831 | lr 2.25e-03\n",
      "\n",
      "=== Local Trial 4/12 | seed=12027 | cfg={'lr': 0.004388741038884002, 'wd': 2.1520115226016903e-06, 'gamma': 3.575297318032362, 'beta': 0.9905754453915928, 'label_smoothing': 0.1, 'clip_norm': 0.5, 'plateau_patience': 8, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0191 | val F1 0.4428 | lr 4.39e-03\n",
      "  Epoch 010 | loss 0.0188 | val F1 0.4360 | lr 4.39e-03\n",
      "  Epoch 020 | loss 0.0161 | val F1 0.4141 | lr 2.19e-03\n",
      "\n",
      "=== Local Trial 5/12 | seed=12028 | cfg={'lr': 0.003011892942352639, 'wd': 4.7204175223384206e-06, 'gamma': 4.0, 'beta': 0.9909447730050431, 'label_smoothing': 0.0, 'clip_norm': 1.0, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0112 | val F1 0.4696 | lr 3.01e-03\n",
      "  Epoch 010 | loss 0.0150 | val F1 0.4864 | lr 3.01e-03\n",
      "  Epoch 020 | loss 0.0116 | val F1 0.4745 | lr 2.11e-03\n",
      "  Epoch 030 | loss 0.0074 | val F1 0.5049 | lr 1.48e-03\n",
      "\n",
      "=== Local Trial 6/12 | seed=12029 | cfg={'lr': 0.005, 'wd': 3.807030156393026e-06, 'gamma': 3.1973768272837884, 'beta': 0.9907853240981145, 'label_smoothing': 0.05, 'clip_norm': 0.5, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0264 | val F1 0.3500 | lr 5.00e-03\n",
      "  Epoch 010 | loss 0.0224 | val F1 0.4629 | lr 5.00e-03\n",
      "  Epoch 020 | loss 0.0097 | val F1 0.4653 | lr 3.50e-03\n",
      "  Epoch 030 | loss 0.0209 | val F1 0.4660 | lr 2.45e-03\n",
      "  Epoch 040 | loss 0.0139 | val F1 0.4447 | lr 1.71e-03\n",
      "\n",
      "=== Local Trial 7/12 | seed=12030 | cfg={'lr': 0.003360686011968521, 'wd': 5.000864084824319e-06, 'gamma': 3.1262989372148864, 'beta': 0.9907783295380935, 'label_smoothing': 0.05, 'clip_norm': 2.0, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0171 | val F1 0.4595 | lr 3.36e-03\n",
      "  Epoch 010 | loss 0.0108 | val F1 0.5169 | lr 3.36e-03\n",
      "  Epoch 020 | loss 0.0063 | val F1 0.5645 | lr 3.36e-03\n",
      "  Epoch 030 | loss 0.0490 | val F1 0.5458 | lr 3.36e-03\n",
      "  Epoch 040 | loss 0.0343 | val F1 0.4201 | lr 2.35e-03\n",
      "\n",
      "=== Local Trial 8/12 | seed=12031 | cfg={'lr': 0.0035204431388237965, 'wd': 3.0886341382872157e-06, 'gamma': 4.0, 'beta': 0.9902563304801222, 'label_smoothing': 0.05, 'clip_norm': 2.0, 'plateau_patience': 12, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0051 | val F1 0.4943 | lr 3.52e-03\n",
      "  Epoch 010 | loss 0.0289 | val F1 0.5379 | lr 3.52e-03\n",
      "  Epoch 020 | loss 0.0201 | val F1 0.4829 | lr 3.52e-03\n",
      "  Epoch 030 | loss 0.0070 | val F1 0.4364 | lr 1.76e-03\n",
      "\n",
      "=== Local Trial 9/12 | seed=12032 | cfg={'lr': 0.00377333642771559, 'wd': 2.0047216776567104e-06, 'gamma': 3.0989170880615284, 'beta': 0.990489657498008, 'label_smoothing': 0.05, 'clip_norm': 1.0, 'plateau_patience': 16, 'plateau_factor': 0.5} ===\n",
      "  Epoch 001 | loss 0.0159 | val F1 0.3871 | lr 3.77e-03\n",
      "  Epoch 010 | loss 0.0481 | val F1 0.4724 | lr 3.77e-03\n",
      "  Epoch 020 | loss 0.0226 | val F1 0.5295 | lr 3.77e-03\n",
      "  Epoch 030 | loss 0.0076 | val F1 0.4655 | lr 3.77e-03\n",
      "  Epoch 040 | loss 0.0131 | val F1 0.4372 | lr 1.89e-03\n",
      "\n",
      "=== Local Trial 10/12 | seed=12033 | cfg={'lr': 0.004849376132461602, 'wd': 3.790036941180753e-06, 'gamma': 3.8324175262803233, 'beta': 0.9904774052196486, 'label_smoothing': 0.1, 'clip_norm': 2.0, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0126 | val F1 0.4627 | lr 4.85e-03\n",
      "  Epoch 010 | loss 0.0419 | val F1 0.4607 | lr 4.85e-03\n",
      "  Epoch 020 | loss 0.0176 | val F1 0.4605 | lr 4.85e-03\n",
      "  Epoch 030 | loss 0.0139 | val F1 0.4759 | lr 3.39e-03\n",
      "\n",
      "=== Local Trial 11/12 | seed=12034 | cfg={'lr': 0.005, 'wd': 4.218762864261317e-06, 'gamma': 3.771398703540339, 'beta': 0.9905928196488366, 'label_smoothing': 0.05, 'clip_norm': 2.0, 'plateau_patience': 12, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0080 | val F1 0.4672 | lr 5.00e-03\n",
      "  Epoch 010 | loss 0.0468 | val F1 0.4673 | lr 5.00e-03\n",
      "  Epoch 020 | loss 0.0248 | val F1 0.4435 | lr 5.00e-03\n",
      "  Epoch 030 | loss 0.0184 | val F1 0.4694 | lr 5.00e-03\n",
      "  Epoch 040 | loss 0.0176 | val F1 0.4986 | lr 5.00e-03\n",
      "  Epoch 050 | loss 0.0053 | val F1 0.5118 | lr 5.00e-03\n",
      "  Epoch 060 | loss 0.0041 | val F1 0.5487 | lr 3.50e-03\n",
      "  Epoch 070 | loss 0.0097 | val F1 0.5129 | lr 3.50e-03\n",
      "\n",
      "=== Local Trial 12/12 | seed=12035 | cfg={'lr': 0.004673076282373883, 'wd': 4.525099018274323e-06, 'gamma': 4.0, 'beta': 0.9903111422190757, 'label_smoothing': 0.1, 'clip_norm': 2.0, 'plateau_patience': 8, 'plateau_factor': 0.7} ===\n",
      "  Epoch 001 | loss 0.0056 | val F1 0.3301 | lr 4.67e-03\n",
      "  Epoch 010 | loss 0.0196 | val F1 0.5516 | lr 4.67e-03\n",
      "  Epoch 020 | loss 0.0537 | val F1 0.4461 | lr 3.27e-03\n",
      "  Epoch 030 | loss 0.0141 | val F1 0.4406 | lr 2.29e-03\n",
      "\n",
      "=== Leaderboard (top 10 by val F1) ===\n",
      " 17 | valF1=0.6152 | testF1=0.4579 | epoch=013 | lr=4.68e-03, wd=3.1e-06, γ=3.34, β=0.9906, ls=0.1, clip=2.0, pat=8, fact=0.7\n",
      " L9 | valF1=0.6015 | testF1=0.4218 | epoch=023 | lr=3.77e-03, wd=2.0e-06, γ=3.10, β=0.9905, ls=0.05, clip=1.0, pat=16, fact=0.5\n",
      " L7 | valF1=0.5648 | testF1=0.4455 | epoch=023 | lr=3.36e-03, wd=5.0e-06, γ=3.13, β=0.9908, ls=0.05, clip=2.0, pat=8, fact=0.7\n",
      " 23 | valF1=0.5628 | testF1=0.4857 | epoch=004 | lr=3.08e-03, wd=6.1e-05, γ=2.31, β=0.9915, ls=0.1, clip=2.0, pat=16, fact=0.7\n",
      " L2 | valF1=0.5602 | testF1=0.4277 | epoch=032 | lr=5.00e-03, wd=4.4e-06, γ=2.83, β=0.9906, ls=0.1, clip=0.5, pat=8, fact=0.7\n",
      "L11 | valF1=0.5592 | testF1=0.5317 | epoch=047 | lr=5.00e-03, wd=4.2e-06, γ=3.77, β=0.9906, ls=0.05, clip=2.0, pat=12, fact=0.7\n",
      " 14 | valF1=0.5579 | testF1=0.4728 | epoch=001 | lr=7.57e-04, wd=3.7e-05, γ=3.12, β=0.9950, ls=0.1, clip=1.0, pat=12, fact=0.5\n",
      "  7 | valF1=0.5579 | testF1=0.4338 | epoch=001 | lr=2.63e-04, wd=1.1e-04, γ=3.79, β=0.9967, ls=0.05, clip=1.0, pat=12, fact=0.5\n",
      " 11 | valF1=0.5579 | testF1=0.4338 | epoch=001 | lr=7.20e-05, wd=4.6e-04, γ=3.96, β=0.9941, ls=0.0, clip=0.5, pat=8, fact=0.5\n",
      " 20 | valF1=0.5579 | testF1=0.4338 | epoch=001 | lr=5.13e-05, wd=6.0e-06, γ=1.43, β=0.9917, ls=0.0, clip=0.5, pat=16, fact=0.5\n",
      "\n",
      "=== Best (by val F1) ===\n",
      "{'lr': 0.004678186464332436, 'wd': 3.14280901334886e-06, 'gamma': 3.342487225772438, 'beta': 0.9905963698507055, 'label_smoothing': 0.1, 'clip_norm': 2.0, 'plateau_patience': 8, 'plateau_factor': 0.7}\n",
      "Best val F1: 0.6152 @ epoch 13 | Test F1: 0.4579 | seed=2040\n",
      "\n",
      "Refit — train on (train ∪ val) with a small shadow validation for early stop\n",
      "  Epoch 001 | loss 0.1848 | val F1 0.6042 | lr 4.68e-03\n",
      "  Epoch 010 | loss 0.0634 | val F1 0.4899 | lr 4.68e-03\n",
      "  Epoch 020 | loss 0.0132 | val F1 0.5479 | lr 3.27e-03\n",
      "  Epoch 030 | loss 0.0159 | val F1 0.8597 | lr 2.29e-03\n",
      "Refit: shadow-val best F1=0.9018 @ epoch 5\n",
      "Final Test F1 after refit: 0.6830\n"
     ]
    }
   ],
   "source": [
    "# === Auto Hyperparam Tuning: Random -> Local Refinement -> Refit (macro-F1) ===\n",
    "# Assumes you already have: model, device, x_dict, edge_index_dict, edge_attr_dict, y, train_mask, val_mask, test_mask\n",
    "\n",
    "import math, random, numpy as np, torch\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ---------------------------\n",
    "# SEARCH SETTINGS\n",
    "# ---------------------------\n",
    "SEARCH_TRIALS_STAGE1 = 24        # initial random search\n",
    "SEARCH_TRIALS_STAGE2 = 12        # local refinement around best\n",
    "BASE_SEED            = 2024\n",
    "MAX_EPOCHS           = 600\n",
    "ES_PATIENCE          = 25\n",
    "ES_MIN_DELTA         = 1e-4\n",
    "MIN_LR               = 1e-6\n",
    "PRINT_EVERY          = 10\n",
    "\n",
    "DO_LOCAL_REFINEMENT  = True\n",
    "DO_REFIT_TRAINVAL    = True\n",
    "REFIT_SHADOW_VAL_FR  = 0.10      # carve this from train+val for early stopping\n",
    "DO_ENSEMBLE_TOPK     = False     # optional: average logits of top-k single-model runs\n",
    "ENSEMBLE_TOPK        = 3\n",
    "ENSEMBLE_REPEATS     = 2         # per top config\n",
    "\n",
    "# If you can rebuild the model (to tune dropout/hidden dim/layers), set this to a factory:\n",
    "# Example:\n",
    "# def MODEL_FACTORY(cfg=None):\n",
    "#     return MyModel(hidden_dim=cfg.get(\"hidden_dim\",128),\n",
    "#                    dropout=cfg.get(\"dropout\",0.3),\n",
    "#                    num_layers=cfg.get(\"num_layers\",3)).to(device)\n",
    "MODEL_FACTORY = None\n",
    "\n",
    "# ---------------------------\n",
    "# UTILITIES\n",
    "# ---------------------------\n",
    "def set_seeds(seed):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def compute_alpha_cb(y_labels, beta, device):\n",
    "    with torch.no_grad():\n",
    "        labeled = y_labels[y_labels >= 0].cpu()\n",
    "        counts  = torch.bincount(labeled)\n",
    "        eff = 1.0 - (beta ** counts.float())\n",
    "        eff = eff.clamp_min(1e-12)\n",
    "        alpha = (1.0 - beta) / eff\n",
    "        alpha = (alpha / alpha.sum() * len(alpha)).to(device)\n",
    "    return alpha\n",
    "\n",
    "def focal_ce_loss(logits, targets, alpha=None, gamma=2.0, label_smoothing=0.0):\n",
    "    ce = F.cross_entropy(\n",
    "        logits, targets, weight=alpha, reduction='none', label_smoothing=label_smoothing\n",
    "    )\n",
    "    pt = torch.softmax(logits, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "    loss = ((1.0 - pt).clamp_min(1e-6) ** gamma) * ce\n",
    "    return loss.mean()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_f1_mask(model, mask):\n",
    "    model.eval()\n",
    "    logits = model(x_dict, edge_index_dict, edge_attr_dict)\n",
    "    pred = logits[mask].argmax(dim=1).cpu().numpy()\n",
    "    true = y[mask].cpu().numpy()\n",
    "    return f1_score(true, pred, average='macro')\n",
    "\n",
    "def build_or_reset_model(init_state, cfg=None):\n",
    "    if MODEL_FACTORY is not None:\n",
    "        return MODEL_FACTORY(cfg)\n",
    "    m = deepcopy(model).to(device)\n",
    "    m.load_state_dict(deepcopy(init_state))\n",
    "    return m\n",
    "\n",
    "def train_one_trial(init_state, config, train_mask_, val_mask_):\n",
    "    m = build_or_reset_model(init_state, config)\n",
    "    opt = torch.optim.AdamW(m.parameters(), lr=config[\"lr\"], weight_decay=config[\"wd\"])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode=\"max\", factor=config[\"plateau_factor\"], patience=config[\"plateau_patience\"],\n",
    "        threshold=ES_MIN_DELTA, cooldown=0, min_lr=MIN_LR\n",
    "    )\n",
    "    alpha_cb = compute_alpha_cb(y, config[\"beta\"], device)\n",
    "\n",
    "    best_f1, best_state, patience, best_epoch = -1.0, None, 0, 0\n",
    "    for epoch in range(1, MAX_EPOCHS + 1):\n",
    "        m.train(True)\n",
    "        logits = m(x_dict, edge_index_dict, edge_attr_dict)\n",
    "        loss = focal_ce_loss(\n",
    "            logits[train_mask_], y[train_mask_],\n",
    "            alpha=alpha_cb, gamma=config[\"gamma\"], label_smoothing=config[\"label_smoothing\"]\n",
    "        )\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(m.parameters(), config[\"clip_norm\"])\n",
    "        opt.step()\n",
    "\n",
    "        val_f1 = eval_f1_mask(m, val_mask_)\n",
    "        scheduler.step(val_f1)\n",
    "\n",
    "        if val_f1 > best_f1 + ES_MIN_DELTA:\n",
    "            best_f1 = val_f1; best_epoch = epoch; patience = 0\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in m.state_dict().items()}\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        if epoch % PRINT_EVERY == 0 or epoch == 1:\n",
    "            print(f\"  Epoch {epoch:03d} | loss {loss.item():.4f} | val F1 {val_f1:.4f} | lr {opt.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        if patience >= ES_PATIENCE:\n",
    "            break\n",
    "\n",
    "    # load best and compute test F1\n",
    "    m.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "    test_f1 = eval_f1_mask(m, test_mask)\n",
    "    return best_f1, test_f1, best_epoch, best_state\n",
    "\n",
    "def loguniform(a, b):\n",
    "    return 10 ** np.random.uniform(np.log10(a), np.log10(b))\n",
    "\n",
    "# ---------------------------\n",
    "# SEARCH SPACES\n",
    "# ---------------------------\n",
    "def sample_config_global():\n",
    "    cfg = {\n",
    "        \"lr\":               float(loguniform(5e-5, 5e-3)),\n",
    "        \"wd\":               float(loguniform(1e-6, 1e-3)),\n",
    "        \"gamma\":            float(np.random.uniform(1.0, 4.0)),\n",
    "        \"beta\":             float(np.random.uniform(0.990, 0.9995)),\n",
    "        \"label_smoothing\":  float(np.random.choice([0.0, 0.05, 0.10])),\n",
    "        \"clip_norm\":        float(np.random.choice([0.5, 1.0, 2.0])),\n",
    "        \"plateau_patience\": int(np.random.choice([8, 12, 16])),\n",
    "        \"plateau_factor\":   float(np.random.choice([0.5, 0.7])),\n",
    "    }\n",
    "    # Optional architecture knobs (only used if MODEL_FACTORY is provided)\n",
    "    if MODEL_FACTORY is not None:\n",
    "        cfg.update({\n",
    "            \"hidden_dim\": int(np.random.choice([64, 128, 256])),\n",
    "            \"dropout\":    float(np.random.choice([0.1, 0.2, 0.3, 0.5])),\n",
    "            \"num_layers\": int(np.random.choice([2, 3, 4])),\n",
    "        })\n",
    "    return cfg\n",
    "\n",
    "def sample_config_local(best_cfg):\n",
    "    # narrow around the best (multiplicative noise for positive params)\n",
    "    def jitter_mult(val, low=0.6, high=1.6): return float(val * np.random.uniform(low, high))\n",
    "    cfg = dict(best_cfg)\n",
    "    cfg[\"lr\"]               = float(np.clip(jitter_mult(best_cfg[\"lr\"]), 5e-5, 5e-3))\n",
    "    cfg[\"wd\"]               = float(np.clip(jitter_mult(best_cfg[\"wd\"]), 1e-6, 1e-3))\n",
    "    cfg[\"gamma\"]            = float(np.clip(np.random.normal(best_cfg[\"gamma\"], 0.4), 1.0, 4.0))\n",
    "    cfg[\"beta\"]             = float(np.clip(np.random.normal(best_cfg[\"beta\"], 3e-4), 0.990, 0.9995))\n",
    "    cfg[\"label_smoothing\"]  = float(np.random.choice([best_cfg[\"label_smoothing\"], 0.0, 0.05, 0.1]))\n",
    "    cfg[\"clip_norm\"]        = float(np.random.choice([best_cfg[\"clip_norm\"], 0.5, 1.0, 2.0]))\n",
    "    cfg[\"plateau_patience\"] = int(np.random.choice([8, 12, 16]))\n",
    "    cfg[\"plateau_factor\"]   = float(np.random.choice([0.5, 0.7]))\n",
    "    if MODEL_FACTORY is not None:\n",
    "        cfg[\"hidden_dim\"] = int(np.random.choice([best_cfg.get(\"hidden_dim\",128), 64, 128, 256]))\n",
    "        cfg[\"dropout\"]    = float(np.clip(np.random.normal(best_cfg.get(\"dropout\",0.3), 0.1), 0.05, 0.6))\n",
    "        cfg[\"num_layers\"] = int(np.random.choice([best_cfg.get(\"num_layers\",3), 2, 3, 4]))\n",
    "    return cfg\n",
    "\n",
    "# ---------------------------\n",
    "# RUN STAGE 1: RANDOM SEARCH\n",
    "# ---------------------------\n",
    "init_state = deepcopy(model.state_dict())\n",
    "set_seeds(BASE_SEED)\n",
    "results = []\n",
    "best_global = {\"val_f1\": -1.0}\n",
    "\n",
    "print(f\"Stage 1 — RANDOM search ({SEARCH_TRIALS_STAGE1} trials)\")\n",
    "for t in range(SEARCH_TRIALS_STAGE1):\n",
    "    trial_seed = BASE_SEED + t\n",
    "    set_seeds(trial_seed)\n",
    "    cfg = sample_config_global()\n",
    "    print(f\"\\n=== Trial {t+1}/{SEARCH_TRIALS_STAGE1} | seed={trial_seed} | cfg={cfg} ===\")\n",
    "    val_f1, test_f1, best_epoch, best_state = train_one_trial(init_state, cfg, train_mask, val_mask)\n",
    "    row = {\"trial\": t+1, \"seed\": trial_seed, \"config\": cfg, \"val_f1\": float(val_f1),\n",
    "           \"test_f1\": float(test_f1), \"best_epoch\": int(best_epoch), \"state\": best_state}\n",
    "    results.append(row)\n",
    "    if val_f1 > best_global[\"val_f1\"] + 1e-9:\n",
    "        best_global = row\n",
    "\n",
    "print(\"\\nTop-5 after Stage 1 (by val F1):\")\n",
    "for r in sorted(results, key=lambda z: (-z[\"val_f1\"], -z[\"test_f1\"]))[:5]:\n",
    "    c = r[\"config\"]\n",
    "    print(f\"  valF1={r['val_f1']:.4f} | testF1={r['test_f1']:.4f} | epoch={r['best_epoch']:03d} | \"\n",
    "          f\"lr={c['lr']:.2e}, wd={c['wd']:.1e}, γ={c['gamma']:.2f}, β={c['beta']:.4f}, \"\n",
    "          f\"ls={c['label_smoothing']}, clip={c['clip_norm']}\")\n",
    "\n",
    "# ---------------------------\n",
    "# RUN STAGE 2: LOCAL REFINEMENT\n",
    "# ---------------------------\n",
    "if DO_LOCAL_REFINEMENT:\n",
    "    print(f\"\\nStage 2 — LOCAL refinement around best (± jitter) with {SEARCH_TRIALS_STAGE2} trials\")\n",
    "    base_cfg = best_global[\"config\"]\n",
    "    for t in range(SEARCH_TRIALS_STAGE2):\n",
    "        trial_seed = BASE_SEED + 10_000 + t\n",
    "        set_seeds(trial_seed)\n",
    "        cfg = sample_config_local(base_cfg)\n",
    "        print(f\"\\n=== Local Trial {t+1}/{SEARCH_TRIALS_STAGE2} | seed={trial_seed} | cfg={cfg} ===\")\n",
    "        val_f1, test_f1, best_epoch, best_state = train_one_trial(init_state, cfg, train_mask, val_mask)\n",
    "        row = {\"trial\": f\"L{t+1}\", \"seed\": trial_seed, \"config\": cfg, \"val_f1\": float(val_f1),\n",
    "               \"test_f1\": float(test_f1), \"best_epoch\": int(best_epoch), \"state\": best_state}\n",
    "        results.append(row)\n",
    "        if val_f1 > best_global[\"val_f1\"] + 1e-9:\n",
    "            best_global = row\n",
    "\n",
    "# ---------------------------\n",
    "# LEADERBOARD\n",
    "# ---------------------------\n",
    "results_sorted = sorted(results, key=lambda r: (-r[\"val_f1\"], -r[\"test_f1\"]))\n",
    "print(\"\\n=== Leaderboard (top 10 by val F1) ===\")\n",
    "for row in results_sorted[:10]:\n",
    "    cfg = row[\"config\"]\n",
    "    print(f\"{str(row['trial']).rjust(3)} | valF1={row['val_f1']:.4f} | testF1={row['test_f1']:.4f} \"\n",
    "          f\"| epoch={row['best_epoch']:03d} | lr={cfg['lr']:.2e}, wd={cfg['wd']:.1e}, \"\n",
    "          f\"γ={cfg['gamma']:.2f}, β={cfg['beta']:.4f}, ls={cfg['label_smoothing']}, \"\n",
    "          f\"clip={cfg['clip_norm']}, pat={cfg.get('plateau_patience',12)}, fact={cfg.get('plateau_factor',0.5)}\")\n",
    "\n",
    "print(\"\\n=== Best (by val F1) ===\")\n",
    "print(best_global[\"config\"])\n",
    "print(f\"Best val F1: {best_global['val_f1']:.4f} @ epoch {best_global['best_epoch']} | \"\n",
    "      f\"Test F1: {best_global['test_f1']:.4f} | seed={best_global['seed']}\")\n",
    "\n",
    "# ---------------------------\n",
    "# OPTIONAL: ENSEMBLE top-k\n",
    "# ---------------------------\n",
    "def logits_all_for_state(state):\n",
    "    m = build_or_reset_model(init_state, best_global[\"config\"])\n",
    "    m.load_state_dict({k: v.to(device) for k, v in state.items()})\n",
    "    m.eval()\n",
    "    with torch.no_grad():\n",
    "        return m(x_dict, edge_index_dict, edge_attr_dict)\n",
    "\n",
    "if DO_ENSEMBLE_TOPK:\n",
    "    topk = results_sorted[:ENSEMBLE_TOPK]\n",
    "    ensemble_logits = None\n",
    "    for i, row in enumerate(topk, 1):\n",
    "        # average several repeats per row (retrain from same config+seed to smooth randomness)\n",
    "        for rep in range(ENSEMBLE_REPEATS):\n",
    "            print(f\"Ensembling: model {i}/{len(topk)}, repeat {rep+1}/{ENSEMBLE_REPEATS}\")\n",
    "            # retrain quickly to get a fresh checkpoint\n",
    "            _val, _test, _ep, _state = train_one_trial(init_state, row[\"config\"], train_mask, val_mask)\n",
    "            log = logits_all_for_state(_state)\n",
    "            ensemble_logits = log if ensemble_logits is None else (ensemble_logits + log)\n",
    "    ensemble_logits /= float(len(topk) * ENSEMBLE_REPEATS)\n",
    "\n",
    "    pred_ens = ensemble_logits[test_mask].argmax(1).cpu().numpy()\n",
    "    true_t   = y[test_mask].cpu().numpy()\n",
    "    test_f1_ens = f1_score(true_t, pred_ens, average='macro')\n",
    "    print(f\"\\nEnsemble Test F1 (top-{ENSEMBLE_TOPK} x{ENSEMBLE_REPEATS}): {test_f1_ens:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# REFIT on TRAIN+VAL (with small shadow-val)\n",
    "# ---------------------------\n",
    "if DO_REFIT_TRAINVAL:\n",
    "    print(\"\\nRefit — train on (train ∪ val) with a small shadow validation for early stop\")\n",
    "    best_cfg = best_global[\"config\"]\n",
    "    # indices of train+val\n",
    "    tv_idx = torch.where((train_mask | val_mask).cpu())[0].cpu().numpy()\n",
    "    y_tv   = y[tv_idx].cpu().numpy()\n",
    "\n",
    "    # stratified shadow split\n",
    "    tv_train_idx, tv_val_idx = train_test_split(tv_idx, test_size=REFIT_SHADOW_VAL_FR,\n",
    "                                                stratify=y_tv, random_state=BASE_SEED+123)\n",
    "    tv_train_mask = torch.zeros_like(train_mask); tv_train_mask[tv_train_idx] = True\n",
    "    tv_val_mask   = torch.zeros_like(val_mask);   tv_val_mask[tv_val_idx]     = True\n",
    "\n",
    "    # train one more time on train+val (shadow-val for early stop)\n",
    "    set_seeds(BASE_SEED+55)\n",
    "    val_f1_refit, test_f1_refit, ep_refit, state_refit = train_one_trial(init_state, best_cfg, tv_train_mask, tv_val_mask)\n",
    "    print(f\"Refit: shadow-val best F1={val_f1_refit:.4f} @ epoch {ep_refit}\")\n",
    "    # final test\n",
    "    m_final = build_or_reset_model(init_state, best_cfg)\n",
    "    m_final.load_state_dict({k: v.to(device) for k, v in state_refit.items()})\n",
    "    final_test_f1 = eval_f1_mask(m_final, test_mask)\n",
    "    print(f\"Final Test F1 after refit: {final_test_f1:.4f}\")\n",
    "\n",
    "# Load best single-model weights by default for downstream use\n",
    "model.load_state_dict({k: v.to(device) for k, v in best_global[\"state\"].items()})\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refit best shadow-val F1 1.0000 @ epoch 2\n",
      "Test macro-F1: 0.5809\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.7000    0.8235        10\n",
      "           1     0.0000    0.0000    0.0000         2\n",
      "           2     0.5000    1.0000    0.6667         7\n",
      "           3     0.9091    0.7692    0.8333        13\n",
      "\n",
      "    accuracy                         0.7500        32\n",
      "   macro avg     0.6023    0.6173    0.5809        32\n",
      "weighted avg     0.7912    0.7500    0.7417        32\n",
      "\n",
      "Confusion matrix (counts):\n",
      " [[ 7  0  2  1]\n",
      " [ 0  0  2  0]\n",
      " [ 0  0  7  0]\n",
      " [ 0  0  3 10]]\n",
      "\n",
      "Confusion matrix (row-normalized):\n",
      " [[0.7   0.    0.2   0.1  ]\n",
      " [0.    0.    1.    0.   ]\n",
      " [0.    0.    1.    0.   ]\n",
      " [0.    0.    0.231 0.769]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mmfs1/home/muhammad.kazim/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mmfs1/home/muhammad.kazim/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mmfs1/home/muhammad.kazim/.local/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# === Finalize best config, refit on train+val (with shadow-val), evaluate on test, save artifacts ===\n",
    "import os, json, numpy as np, torch\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "BEST_CFG = {\n",
    "    \"lr\": 0.004678186464332436,\n",
    "    \"wd\": 3.14280901334886e-06,\n",
    "    \"gamma\": 3.342487225772438,\n",
    "    \"beta\": 0.9905963698507055,\n",
    "    \"label_smoothing\": 0.1,\n",
    "    \"clip_norm\": 2.0,\n",
    "    \"plateau_patience\": 8,\n",
    "    \"plateau_factor\": 0.7,\n",
    "}\n",
    "\n",
    "SEED = 2040           # winner's seed\n",
    "SHADOW_FR = 0.10      # 10% of train+val for early stop\n",
    "MAX_EPOCHS = 800\n",
    "ES_PATIENCE = 30\n",
    "ES_MIN_DELTA = 1e-4\n",
    "MIN_LR = 1e-6\n",
    "\n",
    "# Build shadow split from train ∪ val\n",
    "tv_idx = torch.where((train_mask | val_mask).cpu())[0].cpu().numpy()\n",
    "y_tv   = y[tv_idx].cpu().numpy()\n",
    "tv_train_idx, tv_val_idx = train_test_split(\n",
    "    tv_idx, test_size=SHADOW_FR, stratify=y_tv, random_state=SEED\n",
    ")\n",
    "tv_train_mask = torch.zeros_like(train_mask); tv_train_mask[tv_train_idx] = True\n",
    "tv_val_mask   = torch.zeros_like(val_mask);   tv_val_mask[tv_val_idx]     = True\n",
    "\n",
    "# Train loop (refit with best config)\n",
    "m = build_or_reset_model(init_state, BEST_CFG)\n",
    "opt = torch.optim.AdamW(m.parameters(), lr=BEST_CFG[\"lr\"], weight_decay=BEST_CFG[\"wd\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    opt, mode=\"max\", factor=BEST_CFG[\"plateau_factor\"],\n",
    "    patience=BEST_CFG[\"plateau_patience\"], threshold=ES_MIN_DELTA, min_lr=MIN_LR\n",
    ")\n",
    "alpha_cb = compute_alpha_cb(y, BEST_CFG[\"beta\"], device)\n",
    "\n",
    "best_f1, best_state, patience, best_epoch = -1.0, None, 0, 0\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    m.train(True)\n",
    "    logits = m(x_dict, edge_index_dict, edge_attr_dict)\n",
    "    loss = focal_ce_loss(\n",
    "        logits[tv_train_mask], y[tv_train_mask],\n",
    "        alpha=alpha_cb, gamma=BEST_CFG[\"gamma\"], label_smoothing=BEST_CFG[\"label_smoothing\"]\n",
    "    )\n",
    "    opt.zero_grad(); loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(m.parameters(), BEST_CFG[\"clip_norm\"])\n",
    "    opt.step()\n",
    "\n",
    "    val_f1 = eval_f1_mask(m, tv_val_mask)\n",
    "    scheduler.step(val_f1)\n",
    "\n",
    "    if val_f1 > best_f1 + ES_MIN_DELTA:\n",
    "        best_f1, best_epoch, patience = val_f1, epoch, 0\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in m.state_dict().items()}\n",
    "    else:\n",
    "        patience += 1\n",
    "    if patience >= ES_PATIENCE:\n",
    "        break\n",
    "\n",
    "# Evaluate on test with best state\n",
    "m.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "m.eval()\n",
    "with torch.no_grad():\n",
    "    logits = m(x_dict, edge_index_dict, edge_attr_dict)\n",
    "pred = logits[test_mask].argmax(1).cpu().numpy()\n",
    "true = y[test_mask].cpu().numpy()\n",
    "\n",
    "macro_f1 = f1_score(true, pred, average='macro')\n",
    "report   = classification_report(true, pred, digits=4)\n",
    "cm       = confusion_matrix(true, pred)\n",
    "cm_norm  = confusion_matrix(true, pred, normalize='true')\n",
    "\n",
    "print(f\"Refit best shadow-val F1 {best_f1:.4f} @ epoch {best_epoch}\")\n",
    "print(f\"Test macro-F1: {macro_f1:.4f}\\n\")\n",
    "print(report)\n",
    "print(\"Confusion matrix (counts):\\n\", cm)\n",
    "print(\"\\nConfusion matrix (row-normalized):\\n\", np.round(cm_norm, 3))\n",
    "\n",
    "# Save artifacts (weights, config, confusion matrix)\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "torch.save(best_state, \"artifacts/model_best_state.pt\")\n",
    "with open(\"artifacts/best_config.json\", \"w\") as f:\n",
    "    json.dump(BEST_CFG, f, indent=2)\n",
    "np.savetxt(\"artifacts/confusion_matrix.csv\", cm, fmt=\"%d\", delimiter=\",\")\n",
    "np.savetxt(\"artifacts/confusion_matrix_normalized.csv\", cm_norm, fmt=\"%.6f\", delimiter=\",\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
