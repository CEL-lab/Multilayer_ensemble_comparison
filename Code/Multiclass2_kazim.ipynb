{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Substation Failure Class (Classes_2) with a Heterogeneous GNN\n",
    "We construct a substation–level heterogeneous graph from incidents and lines data, attach a binary target (**Classes_2**), and train a relation–aware GNN that leverages **edge attributes**. We report stratified train/val/test metrics and save reproducible artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — Setup & Repro\n",
    "import os, json, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from torch_geometric.nn import HeteroConv, GATv2Conv\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Paths & Config\n",
    "DATA_PATH   = \"IncidentDataFinal.csv\"      # incidents CSV\n",
    "GRAPH_PATH  = \"Hetero_Final_NW_graph_fixed_kara_labeled.pt\"  # final labeled graph output\n",
    "TARGET_COL  = \"Classes_2\"\n",
    "\n",
    "# optional: where to save trained model + inference config\n",
    "MODEL_OUT   = \"hetero_gatv2_edge_best.pt\"\n",
    "CFG_OUT     = \"inference_config.json\"\n",
    "PRED_OUT    = \"gnn_predictions_by_node.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incidents shape: (264458, 31)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — Load incidents\n",
    "_cols = pd.read_csv(DATA_PATH, nrows=0).columns\n",
    "_date_cols = [c for c in ['Job OFF Time','Job ON Time'] if c in _cols]\n",
    "incident_df = pd.read_csv(DATA_PATH, parse_dates=_date_cols)\n",
    "\n",
    "# Clean the join key to a canonical form (UPPER + strip)\n",
    "if 'Job Substation' not in incident_df.columns:\n",
    "    raise ValueError(\"IncidentDataFinal.csv must include 'Job Substation'.\")\n",
    "incident_df['Job Substation'] = incident_df['Job Substation'].astype(str).str.strip().str.upper()\n",
    "\n",
    "print(\"Incidents shape:\", incident_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_features_clean: (194, 21)\n",
      "       Job Substation  Substation ID          X          Y\n",
      "0  3109:HONOR HEIGHTS           3109 -95.412010  35.771211\n",
      "1      3110:RIVERSIDE           3110 -95.322400  35.762029\n",
      "2    3111:FIVE TRIBES           3111 -95.396671  35.725030\n",
      "3       3114:TENNYSON           3114 -95.396408  35.746585\n",
      "4        3128:HANCOCK           3128 -95.347455  35.724962\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 — Build node table (CSV-aligned)\n",
    "def create_clean_node_features(incident_df):\n",
    "    df = incident_df.copy()\n",
    "\n",
    "    # helper\n",
    "    def mode_or_nan(s):\n",
    "        s = s.dropna()\n",
    "        return s.mode().iloc[0] if not s.empty else np.nan\n",
    "\n",
    "    storm_col = 'Major Storm Event  Y (Yes) or N (No)'\n",
    "    if storm_col in df.columns:\n",
    "        df[storm_col] = (\n",
    "            df[storm_col].astype(str).str.upper().str.strip()\n",
    "              .map({'Y':1,'YES':1,'N':0,'NO':0})\n",
    "        )\n",
    "\n",
    "    agg_plan = {\n",
    "        # Node features\n",
    "        'X': 'mean',\n",
    "        'Y': 'mean',\n",
    "        'Voltage': 'mean',\n",
    "        'Distribution, Substation, Transmission': mode_or_nan,\n",
    "        'Substation ID': mode_or_nan,\n",
    "        'PLANTSTATU': mode_or_nan,\n",
    "        'no_feeders': 'mean',\n",
    "        'no_cities': 'mean',\n",
    "        # Network features (already precomputed in your CSV)\n",
    "        'total_line_length': 'mean',\n",
    "        'avg_line_voltage': 'mean',\n",
    "        'num_connections': 'max',\n",
    "        # Prior incident features\n",
    "        'prior_avg_cust_affected': 'mean',\n",
    "        'prior_avg_downtime': 'mean',\n",
    "        'prior_incident_count': 'mean',\n",
    "        'prior_avg_no_calls': 'mean',\n",
    "        # Incident features\n",
    "        'Call Qty': 'sum',\n",
    "        'Job Duration Mins': 'mean',\n",
    "        'Custs Affected': 'sum',\n",
    "        'Year': mode_or_nan,\n",
    "        storm_col: 'mean'\n",
    "    }\n",
    "\n",
    "    present_aggs = {k:v for k,v in agg_plan.items() if k in df.columns}\n",
    "    node = df.groupby('Job Substation').agg(present_aggs).reset_index()\n",
    "\n",
    "    # de-duplicate any duplicate column names (observed for Job Substation)\n",
    "    node = node.loc[:, ~node.columns.duplicated()]\n",
    "\n",
    "    # order (keep only those present)\n",
    "    #node_features = [\n",
    "    #   'X','Y','Voltage','Distribution, Substation, Transmission',\n",
    "    #  'Job Substation','Substation ID','PLANTSTATU','no_feeders','no_cities'\n",
    "    #]\n",
    "\n",
    "    node_features = [\n",
    "    'X','Y','Voltage','Distribution, Substation, Transmission',\n",
    "    'Substation ID','PLANTSTATU','no_feeders','no_cities'\n",
    "    ]\n",
    "\n",
    "    network_features = ['total_line_length','avg_line_voltage','num_connections']\n",
    "    prior_incident_features = ['prior_avg_cust_affected','prior_avg_downtime',\n",
    "                               'prior_incident_count','prior_avg_no_calls']\n",
    "    incident_features = ['Call Qty','Job Duration Mins','Custs Affected','Year',\n",
    "                         'Major Storm Event  Y (Yes) or N (No)']\n",
    "\n",
    "    desired = (['Job Substation'] + node_features + network_features +\n",
    "               prior_incident_features + incident_features)\n",
    "    node = node[[c for c in desired if c in node.columns]].copy()\n",
    "    return node\n",
    "\n",
    "node_features_clean = create_clean_node_features(incident_df)\n",
    "node_features_clean['Job Substation'] = node_features_clean['Job Substation'].astype(str).str.strip().str.upper()\n",
    "print(\"node_features_clean:\", node_features_clean.shape)\n",
    "print(node_features_clean[['Job Substation','Substation ID','X','Y']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spatial: extracted 7738 edges | columns: ['source', 'target', 'has_line', 'is_nearby', 'line_voltage', 'line_length_km', 'shared_cities', 'shared_feeders', 'distance_km', 'weight']\n",
      "temporal: extracted 19134 edges | columns: ['source', 'target', 'total_weight', 'count']\n",
      "causal: extracted 7192 edges | columns: ['source', 'target', 'z_score', 'cooccur_ratio', 'window_hrs', 'cooccur_count']\n",
      "\n",
      "spatial head:\n",
      "               source          target  has_line  is_nearby  line_voltage  \\\n",
      "0  3109:HONOR HEIGHTS  3110:RIVERSIDE       0.0        1.0           0.0   \n",
      "1  3109:HONOR HEIGHTS   3114:TENNYSON       1.0        1.0          69.0   \n",
      "2  3109:HONOR HEIGHTS    3205:SAPULPA       0.0        1.0           0.0   \n",
      "3  3109:HONOR HEIGHTS      3209:BIXBY       0.0        1.0           0.0   \n",
      "4  3109:HONOR HEIGHTS  7605:DRUMRIGHT       0.0        1.0           0.0   \n",
      "\n",
      "   line_length_km  shared_cities  shared_feeders  distance_km    weight  \n",
      "0        0.000000            0.0             0.0     9.964664  0.091202  \n",
      "1       14.765664            0.0             0.0     1.753879  1.000000  \n",
      "2        0.000000            0.0             0.0    77.578590  0.012726  \n",
      "3        0.000000            0.0             0.0    56.847652  0.017287  \n",
      "4        0.000000            0.0             0.0   132.094879  0.007513  \n",
      "\n",
      "temporal head:\n",
      "               source            target  total_weight  count\n",
      "0  3109:HONOR HEIGHTS    3110:RIVERSIDE     38.945942   43.0\n",
      "1  3109:HONOR HEIGHTS  3111:FIVE TRIBES     26.855967   31.0\n",
      "2  3109:HONOR HEIGHTS     3114:TENNYSON     98.708832  110.0\n",
      "3  3109:HONOR HEIGHTS      3128:HANCOCK     64.068840   71.0\n",
      "4  3109:HONOR HEIGHTS      3205:SAPULPA     24.836361   29.0\n",
      "\n",
      "causal head:\n",
      "              source        target      z_score  cooccur_ratio  window_hrs  \\\n",
      "0       7706:BRISTOW     8706:BOYD   852.334778       1.000000         7.0   \n",
      "1          8158:SARA  8245:COUNCIL   802.584961       0.999810         7.0   \n",
      "2       7208:CYPRESS   7505:WEWOKA  1971.786865       1.000000        25.0   \n",
      "3  7307:LITTLE RIVER   7505:WEWOKA  1089.922607       1.000000        25.0   \n",
      "4         7321:LETHA   7505:WEWOKA  3174.150146       0.999957        25.0   \n",
      "\n",
      "   cooccur_count  \n",
      "0           59.0  \n",
      "1          105.0  \n",
      "2          132.0  \n",
      "3           66.0  \n",
      "4          231.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.461954.bright04/ipykernel_2580804/2355291002.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  g = torch.load(GRAPH_PATH)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Extract edge DataFrames from the loaded graph (PT object)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Ensure the graph is loaded as `g`\n",
    "try:\n",
    "    _ = g['substation'].x\n",
    "except Exception:\n",
    "    g = torch.load(GRAPH_PATH)\n",
    "\n",
    "# Node names (as stored in PT)\n",
    "node_names = [str(n).strip().upper() for n in getattr(g['substation'], 'node_ids', [])]\n",
    "idx_to_name = {i: name for i, name in enumerate(node_names)}\n",
    "\n",
    "def _default_cols(dim):\n",
    "    return [f\"attr_{i}\" for i in range(dim)]\n",
    "\n",
    "def _cols_for(rel, dim):\n",
    "    # Best-effort names based on your build shapes:\n",
    "    # spatial: 8, temporal: 2, causal: 4\n",
    "    if rel == 'spatial' and dim == 8:\n",
    "        return ['has_line','is_nearby','line_voltage','line_length_km',\n",
    "                'shared_cities','shared_feeders','distance_km','weight']\n",
    "    if rel == 'temporal' and dim == 2:\n",
    "        return ['total_weight','count']\n",
    "    if rel == 'causal' and dim == 4:\n",
    "        return ['z_score','cooccur_ratio','window_hrs','cooccur_count']\n",
    "    return _default_cols(dim)\n",
    "\n",
    "edge_frames = {}\n",
    "\n",
    "for rel in g.edge_types:  # e.g., ('substation','spatial','substation')\n",
    "    et_name = rel[1]      # 'spatial' | 'temporal' | 'causal'\n",
    "    store = g[rel]\n",
    "\n",
    "    # edge_index -> source/target names\n",
    "    ei = store.edge_index.detach().cpu().numpy() if hasattr(store, 'edge_index') else np.zeros((2,0), dtype=int)\n",
    "    src_idx, dst_idx = ei[0], ei[1]\n",
    "    src_names = [idx_to_name.get(int(i), f\"IDX_{int(i)}\") for i in src_idx]\n",
    "    dst_names = [idx_to_name.get(int(i), f\"IDX_{int(i)}\") for i in dst_idx]\n",
    "\n",
    "    df = pd.DataFrame({'source': src_names, 'target': dst_names})\n",
    "\n",
    "    # edge_attr -> numeric columns (with best-effort names)\n",
    "    ea = getattr(store, 'edge_attr', None)\n",
    "    if ea is not None and ea.numel() > 0:\n",
    "        ea_np = ea.detach().cpu().numpy()\n",
    "        colnames = _cols_for(et_name, ea_np.shape[1])\n",
    "        # Guard against mismatched counts\n",
    "        if len(colnames) != ea_np.shape[1]:\n",
    "            colnames = _default_cols(ea_np.shape[1])\n",
    "        ea_df = pd.DataFrame(ea_np, columns=colnames)\n",
    "        df = pd.concat([df, ea_df], axis=1)\n",
    "\n",
    "    edge_frames[et_name] = df\n",
    "    print(f\"{et_name}: extracted {len(df)} edges | columns: {list(df.columns)}\")\n",
    "\n",
    "# Expose as variables for later inspection/use\n",
    "spatial_edges  = edge_frames.get('spatial',  pd.DataFrame(columns=['source','target']))\n",
    "temporal_edges = edge_frames.get('temporal', pd.DataFrame(columns=['source','target']))\n",
    "causal_edges   = edge_frames.get('causal',   pd.DataFrame(columns=['source','target']))\n",
    "\n",
    "# Quick peek\n",
    "for nm, edf in [('spatial', spatial_edges), ('temporal', temporal_edges), ('causal', causal_edges)]:\n",
    "    print(f\"\\n{nm} head:\")\n",
    "    print(edf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spatial: provided=7738 | kept=7738\n",
      "temporal: provided=19134 | kept=19134\n",
      "causal: provided=7192 | kept=7192\n",
      "Saved graph -> Hetero_Final_NW_graph_fixed_kara.pt\n",
      "HeteroData(\n",
      "  substation={\n",
      "    x=[194, 18],\n",
      "    node_ids=[194],\n",
      "    substation_id=[194],\n",
      "  },\n",
      "  (substation, spatial, substation)={\n",
      "    edge_index=[2, 7738],\n",
      "    edge_attr=[7738, 8],\n",
      "  },\n",
      "  (substation, temporal, substation)={\n",
      "    edge_index=[2, 19134],\n",
      "    edge_attr=[19134, 2],\n",
      "  },\n",
      "  (substation, causal, substation)={\n",
      "    edge_index=[2, 7192],\n",
      "    edge_attr=[7192, 4],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Build hetero graph keyed by Job Substation names\n",
    "def build_hetero_by_name(node_df, spatial_edges, temporal_edges, causal_edges, save_path):\n",
    "    nd = node_df.copy()\n",
    "    nd['Job Substation'] = nd['Job Substation'].astype(str).str.strip().str.upper()\n",
    "\n",
    "    # numeric node features for x\n",
    "    X_num = (nd.select_dtypes(include=[np.number])\n",
    "               .replace([np.inf, -np.inf], np.nan)\n",
    "               .fillna(0.0))\n",
    "    node_names = nd['Job Substation'].tolist()\n",
    "    name_to_idx = {n:i for i,n in enumerate(node_names)}\n",
    "\n",
    "    data = HeteroData()\n",
    "    data['substation'].x = torch.tensor(X_num.to_numpy(), dtype=torch.float32)\n",
    "    data['substation'].node_ids = node_names\n",
    "    if 'Substation ID' in nd.columns:\n",
    "        data['substation'].substation_id = nd['Substation ID'].tolist()\n",
    "\n",
    "    def _add(df, etype):\n",
    "        keep = df['source'].isin(name_to_idx) & df['target'].isin(name_to_idx)\n",
    "        kept = df.loc[keep].copy()\n",
    "        src = kept['source'].map(name_to_idx).to_numpy()\n",
    "        dst = kept['target'].map(name_to_idx).to_numpy()\n",
    "        ei = torch.tensor(np.vstack([src, dst]), dtype=torch.long)\n",
    "        data['substation', etype, 'substation'].edge_index = ei\n",
    "\n",
    "        ea = (kept.drop(columns=['source','target'], errors='ignore')\n",
    "                   .select_dtypes(include=[np.number])\n",
    "                   .replace([np.inf, -np.inf], np.nan)\n",
    "                   .fillna(0.0))\n",
    "        if ea.shape[1] > 0:\n",
    "            data['substation', etype, 'substation'].edge_attr = torch.tensor(ea.to_numpy(), dtype=torch.float32)\n",
    "        print(f\"{etype}: provided={len(df)} | kept={ei.size(1)}\")\n",
    "\n",
    "    _add(spatial_edges,  'spatial')\n",
    "    _add(temporal_edges, 'temporal')\n",
    "    _add(causal_edges,   'causal')\n",
    "\n",
    "    torch.save(data, save_path)\n",
    "    print(\"Saved graph ->\", save_path)\n",
    "    return data\n",
    "\n",
    "g = build_hetero_by_name(node_features_clean, spatial_edges, temporal_edges, causal_edges,\n",
    "                         save_path=\"Hetero_Final_NW_graph_fixed_kara.pt\")\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attached 'Classes_2' to 194/194 nodes.\n",
      "Saved labeled graph -> Hetero_Final_NW_graph_fixed_kara_labeled.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Attach Classes_2 labels\n",
    "def attach_classes2_by_name(graph, incident_df, target_col=\"Classes_2\", save_path=None):\n",
    "    g = graph\n",
    "    sub = g['substation']\n",
    "\n",
    "    df = incident_df.copy()\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Missing '{target_col}' in incidents.\")\n",
    "    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n",
    "    df['Job Substation'] = df['Job Substation'].astype(str).str.strip().str.upper()\n",
    "\n",
    "    labels_by_name = (\n",
    "        df.groupby('Job Substation')[target_col]\n",
    "          .apply(lambda s: s.dropna().round().astype(int).mode().iloc[0] if s.dropna().size else np.nan)\n",
    "    )\n",
    "\n",
    "    node_names = [str(n).strip().upper() for n in sub.node_ids]\n",
    "    y_list, mask_list = [], []\n",
    "    for name in node_names:\n",
    "        val = labels_by_name.get(name, np.nan)\n",
    "        if pd.isna(val):\n",
    "            y_list.append(-1); mask_list.append(False)\n",
    "        else:\n",
    "            y_list.append(int(val)); mask_list.append(True)\n",
    "\n",
    "    sub.y = torch.tensor(y_list, dtype=torch.long)\n",
    "    sub.train_mask = torch.tensor(mask_list, dtype=torch.bool)\n",
    "\n",
    "    labeled = int(sub.train_mask.sum().item())\n",
    "    print(f\"Attached '{target_col}' to {labeled}/{len(node_names)} nodes.\")\n",
    "    if save_path:\n",
    "        torch.save(g, save_path); print(\"Saved labeled graph ->\", save_path)\n",
    "    return g\n",
    "\n",
    "g = attach_classes2_by_name(g, incident_df, TARGET_COL, save_path=GRAPH_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.461954.bright04/ipykernel_2580804/1225572428.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  g = torch.load(GRAPH_PATH).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial edges undirected -> 15476\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Prep: dicts + normalization + (optional) undirected spatial\n",
    "g = torch.load(GRAPH_PATH).to(device)\n",
    "\n",
    "edge_index_dict = {rel: g[rel].edge_index.to(device) for rel in g.edge_types}\n",
    "edge_attr_dict  = {}\n",
    "edge_dim_dict   = {}\n",
    "for rel in g.edge_types:\n",
    "    ea = getattr(g[rel], 'edge_attr', None)\n",
    "    if ea is not None and ea.numel() > 0:\n",
    "        mean, std = ea.mean(dim=0, keepdim=True), ea.std(dim=0, keepdim=True)\n",
    "        std[std == 0] = 1.0\n",
    "        edge_attr_dict[rel] = ((ea - mean) / std).to(device)\n",
    "        edge_dim_dict[rel]  = ea.size(1)\n",
    "    else:\n",
    "        edge_attr_dict[rel] = None\n",
    "        edge_dim_dict[rel]  = 0\n",
    "\n",
    "# Node features normalization (z-score)\n",
    "x_raw = g['substation'].x.to(device)\n",
    "x_mean = x_raw.mean(dim=0, keepdim=True)\n",
    "x_std  = x_raw.std(dim=0, keepdim=True); x_std[x_std==0] = 1.0\n",
    "x_norm = (x_raw - x_mean) / x_std\n",
    "x_dict = {'substation': x_norm}\n",
    "\n",
    "y = g['substation'].y.to(device)\n",
    "num_nodes = x_norm.size(0)\n",
    "\n",
    "# Make SPATIAL edges undirected (duplicate reverse edges)\n",
    "if ('substation','spatial','substation') in g.edge_types:\n",
    "    rel = ('substation','spatial','substation')\n",
    "    ei = edge_index_dict[rel]\n",
    "    ea = edge_attr_dict[rel]\n",
    "    rev_ei = torch.stack([ei[1], ei[0]], dim=0)\n",
    "    edge_index_dict[rel] = torch.cat([ei, rev_ei], dim=1)\n",
    "    if ea is not None:\n",
    "        edge_attr_dict[rel] = torch.cat([ea, ea], dim=0)\n",
    "    print(\"Spatial edges undirected ->\", edge_index_dict[rel].size(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts: {0: 104, 1: 90}\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 — Splits & weights\n",
    "idx = np.arange(num_nodes)\n",
    "y_np = y.detach().cpu().numpy()\n",
    "\n",
    "train_idx, tmp_idx = train_test_split(idx, test_size=0.30, stratify=y_np, random_state=SEED)\n",
    "val_idx, test_idx   = train_test_split(tmp_idx, test_size=0.50, stratify=y_np[tmp_idx], random_state=SEED)\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool, device=device); train_mask[train_idx] = True\n",
    "val_mask   = torch.zeros(num_nodes, dtype=torch.bool, device=device); val_mask[val_idx]   = True\n",
    "test_mask  = torch.zeros(num_nodes, dtype=torch.bool, device=device); test_mask[test_idx]  = True\n",
    "\n",
    "counts = np.bincount(y_np)\n",
    "class_weights = torch.tensor(counts.max() / counts, dtype=torch.float32, device=device)\n",
    "print(\"Class counts:\", dict(enumerate(counts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 — Model\n",
    "class HeteroGATv2Edge(nn.Module):\n",
    "    def __init__(self, hidden=64, out_channels=2, metadata=None, heads=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        conv1_dict, conv2_dict = {}, {}\n",
    "        for rel in metadata[1]:\n",
    "            edim = edge_dim_dict.get(rel, 0)\n",
    "            if edim > 0:\n",
    "                conv1 = GATv2Conv((-1,-1), hidden, heads=heads, concat=False,\n",
    "                                  edge_dim=edim, add_self_loops=False, dropout=dropout)\n",
    "                conv2 = GATv2Conv((-1,-1), hidden, heads=heads, concat=False,\n",
    "                                  edge_dim=edim, add_self_loops=False, dropout=dropout)\n",
    "            else:\n",
    "                conv1 = GATv2Conv((-1,-1), hidden, heads=heads, concat=False,\n",
    "                                  add_self_loops=False, dropout=dropout)\n",
    "                conv2 = GATv2Conv((-1,-1), hidden, heads=heads, concat=False,\n",
    "                                  add_self_loops=False, dropout=dropout)\n",
    "            conv1_dict[rel] = conv1\n",
    "            conv2_dict[rel] = conv2\n",
    "        self.conv1 = HeteroConv(conv1_dict, aggr='mean')\n",
    "        self.conv2 = HeteroConv(conv2_dict, aggr='mean')\n",
    "        self.lin   = nn.Linear(hidden, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_attr_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict, edge_attr_dict=edge_attr_dict)\n",
    "        x_dict = {k: F.relu(v) for k,v in x_dict.items()}\n",
    "        x_dict = {k: F.dropout(v, p=self.dropout, training=self.training) for k,v in x_dict.items()}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict, edge_attr_dict=edge_attr_dict)\n",
    "        x_dict = {k: F.relu(v) for k,v in x_dict.items()}\n",
    "        return self.lin(x_dict['substation'])\n",
    "\n",
    "model = HeteroGATv2Edge(hidden=64, out_channels=2, metadata=g.metadata(), heads=2, dropout=0.2).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', patience=8, factor=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | loss 0.3464 | val F1 0.7238\n",
      "Epoch 010 | loss 0.3109 | val F1 0.7238\n",
      "Epoch 020 | loss 0.3304 | val F1 0.7238\n",
      "\n",
      "Best val F1: 0.7238 @ epoch 1 | Test F1 (0.50): 0.6970\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 — Train\n",
    "def train_step():\n",
    "    model.train()\n",
    "    logits = model(x_dict, edge_index_dict, edge_attr_dict)\n",
    "    loss = F.cross_entropy(logits[train_mask], y[train_mask], weight=class_weights, label_smoothing=0.05)\n",
    "    opt.zero_grad(); loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    opt.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_f1(mask):\n",
    "    model.eval()\n",
    "    logits = model(x_dict, edge_index_dict, edge_attr_dict)\n",
    "    pred = logits[mask].argmax(1).cpu().numpy()\n",
    "    true = y[mask].cpu().numpy()\n",
    "    return f1_score(true, pred, average='macro')\n",
    "\n",
    "best_f1, best_state, patience, best_epoch = -1, None, 0, 0\n",
    "for epoch in range(1, 151):\n",
    "    loss = train_step()\n",
    "    val_f1 = eval_f1(val_mask)\n",
    "    scheduler.step(val_f1)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1, best_state, patience, best_epoch = val_f1, {k:v.cpu() for k,v in model.state_dict().items()}, 0, epoch\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | loss {loss:.4f} | val F1 {val_f1:.4f}\")\n",
    "    if patience >= 20:  # early stop\n",
    "        break\n",
    "\n",
    "# Load best and evaluate test (argmax @ 0.50)\n",
    "model.load_state_dict({k:v.to(device) for k,v in best_state.items()})\n",
    "logits = model(x_dict, edge_index_dict, edge_attr_dict)\n",
    "pred_test = logits[test_mask].argmax(1).cpu().numpy()\n",
    "true_test = y[test_mask].cpu().numpy()\n",
    "test_f1 = f1_score(true_test, pred_test, average='macro')\n",
    "\n",
    "print(f\"\\nBest val F1: {best_f1:.4f} @ epoch {best_epoch} | Test F1 (0.50): {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val F1 via thresholding: 0.7586 at t=0.325\n",
      "Test F1 at tuned threshold:  0.7285\n"
     ]
    }
   ],
   "source": [
    "# Cell 12 — Threshold tuning\n",
    "@torch.no_grad()\n",
    "def logits_all():\n",
    "    model.eval()\n",
    "    return model(x_dict, edge_index_dict, edge_attr_dict)\n",
    "\n",
    "log = logits_all()\n",
    "probs = torch.softmax(log, dim=1)[:,1].cpu().numpy()\n",
    "y_true = y.cpu().numpy()\n",
    "val_mask_np = val_mask.cpu().numpy()\n",
    "test_mask_np = test_mask.cpu().numpy()\n",
    "\n",
    "best_t, best_val = 0.5, -1\n",
    "for t in np.linspace(0.2, 0.8, 25):\n",
    "    f1v = f1_score(y_true[val_mask_np], (probs[val_mask_np] >= t).astype(int), average='macro')\n",
    "    if f1v > best_val:\n",
    "        best_val, best_t = f1v, t\n",
    "\n",
    "pred_test_thr = (probs[test_mask_np] >= best_t).astype(int)\n",
    "test_f1_thr = f1_score(y_true[test_mask_np], pred_test_thr, average='macro')\n",
    "\n",
    "print(f\"Best val F1 via thresholding: {best_val:.4f} at t={best_t:.3f}\")\n",
    "print(f\"Test F1 at tuned threshold:  {test_f1_thr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Val @ tuned t ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8462    0.6875    0.7586        16\n",
      "           1     0.6875    0.8462    0.7586        13\n",
      "\n",
      "    accuracy                         0.7586        29\n",
      "   macro avg     0.7668    0.7668    0.7586        29\n",
      "weighted avg     0.7750    0.7586    0.7586        29\n",
      "\n",
      "[[11  5]\n",
      " [ 2 11]]\n",
      "\n",
      "== Test @ tuned t ==\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9000    0.5625    0.6923        16\n",
      "           1     0.6500    0.9286    0.7647        14\n",
      "\n",
      "    accuracy                         0.7333        30\n",
      "   macro avg     0.7750    0.7455    0.7285        30\n",
      "weighted avg     0.7833    0.7333    0.7261        30\n",
      "\n",
      "[[ 9  7]\n",
      " [ 1 13]]\n"
     ]
    }
   ],
   "source": [
    "# Cell 13 — Reports\n",
    "pred_val_thr = (probs[val_mask_np]  >= best_t).astype(int)\n",
    "pred_test_thr= (probs[test_mask_np] >= best_t).astype(int)\n",
    "\n",
    "print(\"== Val @ tuned t ==\")\n",
    "print(classification_report(y_true[val_mask_np],  pred_val_thr,  digits=4))\n",
    "print(confusion_matrix(y_true[val_mask_np],  pred_val_thr))\n",
    "\n",
    "print(\"\\n== Test @ tuned t ==\")\n",
    "print(classification_report(y_true[test_mask_np], pred_test_thr, digits=4))\n",
    "print(confusion_matrix(y_true[test_mask_np], pred_test_thr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: hetero_gatv2_edge_best.pt inference_config.json gnn_predictions_by_node.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 14 — Save artifacts\n",
    "sub = g['substation']\n",
    "node_names = [str(n) for n in getattr(sub, 'node_ids', [f\"node_{i}\" for i in range(len(probs))])]\n",
    "sub_ids = getattr(sub, 'substation_id', [None]*len(node_names))\n",
    "\n",
    "def split_tag(i):\n",
    "    if train_mask[i]: return \"train\"\n",
    "    if val_mask[i]:   return \"val\"\n",
    "    if test_mask[i]:  return \"test\"\n",
    "    return \"unused\"\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    \"node_idx\": np.arange(len(probs)),\n",
    "    \"Job Substation\": node_names,\n",
    "    \"Substation ID\": sub_ids,\n",
    "    \"y_true\": y_true,\n",
    "    \"prob_1\": probs,\n",
    "    \"pred@0.50\": (probs >= 0.50).astype(int),\n",
    "    f\"pred@{best_t:.3f}\": (probs >= best_t).astype(int),\n",
    "    \"split\": [split_tag(i) for i in range(len(probs))]\n",
    "})\n",
    "pred_df.to_csv(PRED_OUT, index=False)\n",
    "\n",
    "best_state_cpu = {k:v.cpu() for k,v in best_state.items()}\n",
    "torch.save(best_state_cpu, MODEL_OUT)\n",
    "\n",
    "cfg = {\n",
    "    \"threshold\": float(best_t),\n",
    "    \"x_mean\": x_mean.detach().cpu().numpy().tolist(),\n",
    "    \"x_std\":  x_std.detach().cpu().numpy().tolist(),\n",
    "    \"edge_dims\": {str(rel): int(edge_dim_dict[rel]) for rel in edge_dim_dict},\n",
    "    \"metadata\": {\"node_types\": g.node_types, \"edge_types\": [tuple(r) for r in g.edge_types]},\n",
    "    \"graph_path\": GRAPH_PATH,\n",
    "    \"model_path\": MODEL_OUT,\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "with open(CFG_OUT, \"w\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", MODEL_OUT, CFG_OUT, PRED_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      none -> Val F1 0.7586 | Test F1 0.7285\n",
      "drop:spatial -> Val F1 0.3095 | Test F1 0.3182\n",
      "drop:temporal -> Val F1 0.6836 | Test F1 0.6528\n",
      "drop:causal -> Val F1 0.5894 | Test F1 0.4994\n"
     ]
    }
   ],
   "source": [
    "# Cell 15 — Ablation (no retrain; mask relations at inference)\n",
    "@torch.no_grad()\n",
    "def eval_with_mask(disable=set()):\n",
    "    masked_ei = {}\n",
    "    masked_ea = {}\n",
    "    for rel in g.edge_types:\n",
    "        if rel in disable:\n",
    "            masked_ei[rel] = torch.empty((2,0), dtype=edge_index_dict[rel].dtype, device=device)\n",
    "            ea = edge_attr_dict[rel]\n",
    "            masked_ea[rel] = None if ea is None else torch.empty((0, ea.size(1)), dtype=ea.dtype, device=device)\n",
    "        else:\n",
    "            masked_ei[rel] = edge_index_dict[rel]\n",
    "            masked_ea[rel] = edge_attr_dict[rel]\n",
    "    logits = model(x_dict, masked_ei, masked_ea)\n",
    "    p = torch.softmax(logits, dim=1)[:,1].cpu().numpy()\n",
    "    pred = (p >= best_t).astype(int)\n",
    "    f1v  = f1_score(y_true[val_mask_np],  pred[val_mask_np],  average='macro')\n",
    "    f1t  = f1_score(y_true[test_mask_np], pred[test_mask_np], average='macro')\n",
    "    return f1v, f1t\n",
    "\n",
    "for drop in [set(),\n",
    "             {('substation','spatial','substation')},\n",
    "             {('substation','temporal','substation')},\n",
    "             {('substation','causal','substation')}]:\n",
    "    tag = \"none\" if not drop else \"drop:\" + \",\".join([f\"{r[1]}\" for r in drop])\n",
    "    f1v, f1t = eval_with_mask(drop)\n",
    "    print(f\"{tag:>10} -> Val F1 {f1v:.4f} | Test F1 {f1t:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "- Nodes: 194 substations; features aggregated per `Job Substation` from IncidentDataFinal.csv.\n",
    "- Edges:\n",
    "  - spatial: connectivity/nearby with attributes (e.g., `line_length_km`, `distance_km`, etc.), made **undirected** at training time.\n",
    "  - temporal: directed co-variation edges (`total_weight`, `count`).\n",
    "  - causal: directed cause co-occurrence edges with `z_score`, `cooccur_count`, `cooccur_ratio`, `window_hrs`.\n",
    "- Target: `Classes_2` (majority label per substation).\n",
    "- Split: stratified 70/15/15 over nodes.\n",
    "- Model: 2-layer Hetero GATv2 with per-relation `edge_dim`, mean aggregation, feature/edge z-scoring, label smoothing, grad clipping.\n",
    "- Threshold tuning on the validation set improves macro-F1 on the test set.\n",
    "- Artifacts saved for reproducibility: model weights, normalization parameters, tuned threshold, predictions CSV, and metadata.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
