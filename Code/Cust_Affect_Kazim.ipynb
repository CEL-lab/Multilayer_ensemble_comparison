{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Substation Classification (Cust_Class) with Hetero GNN\n",
    "We predict **Cust_Class** (0/1) at the substation level using the existing heterogeneous graph.\n",
    "Labels are attached by majority vote per *Job Substation* (rows with labels not in {0,1} are dropped).\n",
    "Training uses a robust stratified split and classâ€‘balanced focal loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup & Paths\n",
    "import os, json, random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from torch_geometric.nn import HeteroConv, GATv2Conv\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "DATA_PATH = 'IncidentDataFinal.csv'\n",
    "GRAPH_IN  = 'Hetero_Final_NW_graph_fixed_kara.pt'      # unlabeled graph you already built\n",
    "GRAPH_OUT = 'Hetero_graph_kara_CustClass_labeled.pt'   # will save labeled (0/1 only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incidents: (264458, 31)\n",
      "HeteroData(\n",
      "  substation={\n",
      "    x=[194, 18],\n",
      "    node_ids=[194],\n",
      "    substation_id=[194],\n",
      "  },\n",
      "  (substation, spatial, substation)={\n",
      "    edge_index=[2, 7738],\n",
      "    edge_attr=[7738, 8],\n",
      "  },\n",
      "  (substation, temporal, substation)={\n",
      "    edge_index=[2, 19134],\n",
      "    edge_attr=[19134, 2],\n",
      "  },\n",
      "  (substation, causal, substation)={\n",
      "    edge_index=[2, 7192],\n",
      "    edge_attr=[7192, 4],\n",
      "  }\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.461954.bright04/ipykernel_2581620/3085901278.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  g = torch.load(GRAPH_IN)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load incidents (CSV) and base graph\n",
    "_cols = pd.read_csv(DATA_PATH, nrows=0).columns\n",
    "_date_cols = [c for c in ['Job OFF Time','Job ON Time'] if c in _cols]\n",
    "inc = pd.read_csv(DATA_PATH, parse_dates=_date_cols)\n",
    "\n",
    "g = torch.load(GRAPH_IN)\n",
    "print('Incidents:', inc.shape)\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled counts (0/1): {0: 194}\n",
      "Labeled (0/1) nodes: 194/194\n",
      "Saved -> Hetero_graph_kara_CustClass_labeled.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Attach Cust_Class (keep only 0/1), majority per substation, save labeled graph\n",
    "def attach_binary_label(graph, incident_df, target_col='Cust_Class', save_path=None):\n",
    "    g = graph\n",
    "    sub = g['substation']\n",
    "\n",
    "    df = incident_df.copy()\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Missing '{target_col}' in incidents.\")\n",
    "    df['Job Substation'] = df['Job Substation'].astype(str).str.strip().str.upper()\n",
    "    df[target_col] = pd.to_numeric(df[target_col], errors='coerce')\n",
    "\n",
    "    # keep only labels 0/1\n",
    "    df = df[df[target_col].isin([0,1])]\n",
    "\n",
    "    labels_by_name = (\n",
    "        df.groupby('Job Substation')[target_col]\n",
    "          .apply(lambda s: s.dropna().astype(int).mode().iloc[0] if s.dropna().size else np.nan)\n",
    "    )\n",
    "\n",
    "    node_names = [str(n).strip().upper() for n in getattr(sub, 'node_ids', [])]\n",
    "    y_list, mask_list = [], []\n",
    "    for name in node_names:\n",
    "        v = labels_by_name.get(name, np.nan)\n",
    "        if pd.isna(v): y_list.append(-1); mask_list.append(False)\n",
    "        else:          y_list.append(int(v)); mask_list.append(True)\n",
    "\n",
    "    sub.y = torch.tensor(y_list, dtype=torch.long)\n",
    "    sub.train_mask = torch.tensor(mask_list, dtype=torch.bool)\n",
    "\n",
    "    labeled = int(sub.train_mask.sum().item())\n",
    "    if labeled:\n",
    "        yy = np.array(y_list)[np.array(mask_list)]\n",
    "        vals, cnts = np.unique(yy, return_counts=True)\n",
    "        print('Labeled counts (0/1):', dict(zip(vals.tolist(), cnts.tolist())))\n",
    "    print(f\"Labeled (0/1) nodes: {labeled}/{len(node_names)}\")\n",
    "\n",
    "    if save_path:\n",
    "        torch.save(g, save_path); print('Saved ->', save_path)\n",
    "    return g\n",
    "\n",
    "g = attach_binary_label(g, inc, 'Cust_Class', save_path=GRAPH_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/pbs.460460.bright04/ipykernel_3051168/1590346454.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  g = torch.load(GRAPH_OUT).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial edges (undirected): 15476\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build tensors, normalize, make spatial undirected\n",
    "g = torch.load(GRAPH_OUT).to(device)\n",
    "\n",
    "# Edge dicts + per-relation z-score normalization\n",
    "edge_index_dict = {rel: g[rel].edge_index.to(device) for rel in g.edge_types}\n",
    "edge_attr_dict, edge_dim_dict = {}, {}\n",
    "for rel in g.edge_types:\n",
    "    ea = getattr(g[rel], 'edge_attr', None)\n",
    "    if ea is not None and ea.numel() > 0:\n",
    "        m, s = ea.mean(0, keepdim=True), ea.std(0, keepdim=True)\n",
    "        s[s==0] = 1.0\n",
    "        edge_attr_dict[rel] = ((ea - m) / s).to(device)\n",
    "        edge_dim_dict[rel]  = ea.size(1)\n",
    "    else:\n",
    "        edge_attr_dict[rel] = None\n",
    "        edge_dim_dict[rel]  = 0\n",
    "\n",
    "# Node features z-score\n",
    "x_raw = g['substation'].x.to(device)\n",
    "x_mean, x_std = x_raw.mean(0, keepdim=True), x_raw.std(0, keepdim=True)\n",
    "x_std[x_std==0] = 1.0\n",
    "x_dict = {'substation': (x_raw - x_mean) / x_std}\n",
    "\n",
    "# Labels\n",
    "y = g['substation'].y.to(device)\n",
    "num_nodes = x_dict['substation'].size(0)\n",
    "\n",
    "# Make spatial undirected; keep temporal/causal directed\n",
    "if ('substation','spatial','substation') in g.edge_types:\n",
    "    rel = ('substation','spatial','substation')\n",
    "    ei = edge_index_dict[rel]; ea = edge_attr_dict[rel]\n",
    "    rev_ei = torch.stack([ei[1], ei[0]], dim=0)\n",
    "    edge_index_dict[rel] = torch.cat([ei, rev_ei], dim=1)\n",
    "    if ea is not None:\n",
    "        edge_attr_dict[rel] = torch.cat([ea, ea], dim=0)\n",
    "    print('Spatial edges (undirected):', edge_index_dict[rel].size(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled per-class counts: {0: 194}\n",
      "Used split seed: 42\n",
      "train size=155 | class counts: {0: 155}\n",
      "val   size= 19 | class counts: {0: 19}\n",
      "test  size= 20 | class counts: {0: 20}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Robust stratified split with per-class minimums\n",
    "def split_counts(mask, name):\n",
    "    yy = y[mask].cpu().numpy()\n",
    "    c  = dict(Counter(yy))\n",
    "    print(f\"{name:5} size={int(mask.sum().item()):3d} | class counts:\", c)\n",
    "\n",
    "def _check_feasibility(y_all, labeled_idx, train_frac, val_frac, test_frac, min_per_class):\n",
    "    counts = Counter(y_all[labeled_idx])\n",
    "    p_rest = 1.0 - train_frac\n",
    "    p_val  = val_frac / (val_frac + test_frac)\n",
    "    p_test = test_frac / (val_frac + test_frac)\n",
    "    need_val  = {c: math.ceil((min_per_class)/(p_rest*p_val)  + 1e-9) for c in counts}\n",
    "    need_test = {c: math.ceil((min_per_class)/(p_rest*p_test) + 1e-9) for c in counts}\n",
    "    infeasible = {c: (counts[c], max(need_val[c], need_test[c])) for c in counts if counts[c] < max(need_val[c], need_test[c])}\n",
    "    return counts, infeasible\n",
    "\n",
    "def stratified_split_with_min(y_all, labeled_idx, train_frac=0.8, val_frac=0.1, test_frac=0.1,\n",
    "                              min_per_class=2, base_seed=42, max_tries=500):\n",
    "    assert abs(train_frac + val_frac + test_frac - 1.0) < 1e-6\n",
    "    counts, infeasible = _check_feasibility(y_all, labeled_idx, train_frac, val_frac, test_frac, min_per_class)\n",
    "    if infeasible:\n",
    "        raise RuntimeError('Split infeasible for some classes; reduce min_per_class or adjust fractions.')\n",
    "\n",
    "    y_lab = y_all[labeled_idx]\n",
    "    for t in range(max_tries):\n",
    "        seed = base_seed + t\n",
    "        tr_idx, rest_idx = train_test_split(\n",
    "            labeled_idx, test_size=(1 - train_frac), stratify=y_lab, random_state=seed\n",
    "        )\n",
    "        pos_map = {nid:i for i, nid in enumerate(labeled_idx)}\n",
    "        y_rest  = np.array([y_lab[pos_map[nid]] for nid in rest_idx])\n",
    "\n",
    "        val_idx, test_idx = train_test_split(\n",
    "            rest_idx, test_size=test_frac/(val_frac+test_frac),\n",
    "            stratify=y_rest, random_state=seed\n",
    "        )\n",
    "        ok_val  = all(v >= min_per_class for v in Counter(y_all[val_idx]).values())\n",
    "        ok_test = all(v >= min_per_class for v in Counter(y_all[test_idx]).values())\n",
    "        if ok_val and ok_test:\n",
    "            return tr_idx, val_idx, test_idx, seed\n",
    "    raise RuntimeError('Could not find a split meeting per-class minimums.')\n",
    "\n",
    "# Labeled nodes only\n",
    "labeled_idx  = torch.where(y >= 0)[0].cpu().numpy()\n",
    "y_np         = y.cpu().numpy()\n",
    "\n",
    "counts, infeasible = _check_feasibility(y_np, labeled_idx, 0.8, 0.1, 0.1, min_per_class=2)\n",
    "print('Labeled per-class counts:', dict(counts))\n",
    "\n",
    "train_idx_l, val_idx_l, test_idx_l, used_seed = stratified_split_with_min(\n",
    "    y_np, labeled_idx, train_frac=0.8, val_frac=0.1, test_frac=0.1, min_per_class=2, base_seed=SEED\n",
    ")\n",
    "\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool, device=device); train_mask[torch.tensor(train_idx_l, device=device)] = True\n",
    "val_mask   = torch.zeros(num_nodes, dtype=torch.bool, device=device); val_mask[torch.tensor(val_idx_l, device=device)]   = True\n",
    "test_mask  = torch.zeros(num_nodes, dtype=torch.bool, device=device); test_mask[torch.tensor(test_idx_l, device=device)]  = True\n",
    "\n",
    "print('Used split seed:', used_seed)\n",
    "split_counts(train_mask, 'train')\n",
    "split_counts(val_mask,   'val')\n",
    "split_counts(test_mask,  'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class-balanced alpha: [1.0407374e-06 1.9999990e+00] | counts: [194.   0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Class-balanced focal for binary\n",
    "def class_balanced_alpha(labels, num_classes=2, beta=0.9999):\n",
    "    counts = np.bincount(labels, minlength=num_classes).astype(np.float32)\n",
    "    effective_num = 1.0 - np.power(beta, counts)\n",
    "    weights = (1.0 - beta) / np.maximum(effective_num, 1e-8)\n",
    "    weights = weights / weights.sum() * num_classes\n",
    "    return torch.tensor(weights, dtype=torch.float32, device=device), counts\n",
    "\n",
    "ALPHA_CB, cls_counts = class_balanced_alpha(y_np[labeled_idx], num_classes=2, beta=0.9999)\n",
    "print('Class-balanced alpha:', ALPHA_CB.detach().cpu().numpy(), '| counts:', cls_counts)\n",
    "\n",
    "def focal_ce_loss(logits, targets, alpha=None, gamma=2.0):\n",
    "    logp = F.log_softmax(logits, dim=1)\n",
    "    p    = torch.exp(logp)\n",
    "    ce   = F.nll_loss(logp, targets, reduction='none')\n",
    "    pt   = p[torch.arange(p.size(0), device=logits.device), targets]\n",
    "    loss = ((1 - pt) ** gamma) * ce\n",
    "    if alpha is not None:\n",
    "        loss = alpha[targets] * loss\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model (GATv2 + edge_dim), binary (2 logits)\n",
    "class HeteroGATv2Edge(nn.Module):\n",
    "    def __init__(self, hidden=128, out_channels=2, metadata=None, heads=4, dropout=0.30):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        conv1_dict, conv2_dict = {}, {}\n",
    "        for rel in metadata[1]:\n",
    "            edim = edge_dim_dict.get(rel, 0)\n",
    "            if edim > 0:\n",
    "                conv1 = GATv2Conv((-1,-1), hidden, heads=heads, concat=False,\n",
    "                                  edge_dim=edim, add_self_loops=False, dropout=dropout)\n",
    "                conv2 = GATv2Conv((-1,-1), hidden, heads=heads, concat=False,\n",
    "                                  edge_dim=edim, add_self_loops=False, dropout=dropout)\n",
    "            else:\n",
    "                conv1 = GATv2Conv((-1,-1), hidden, heads=heads, concat=False,\n",
    "                                  add_self_loops=False, dropout=dropout)\n",
    "                conv2 = GATv2Conv((-1,-1), hidden, heads=heads, concat=False,\n",
    "                                  add_self_loops=False, dropout=dropout)\n",
    "            conv1_dict[rel] = conv1; conv2_dict[rel] = conv2\n",
    "        self.conv1 = HeteroConv(conv1_dict, aggr='mean')\n",
    "        self.conv2 = HeteroConv(conv2_dict, aggr='mean')\n",
    "        self.bn1   = nn.BatchNorm1d(hidden)\n",
    "        self.bn2   = nn.BatchNorm1d(hidden)\n",
    "        self.lin   = nn.Linear(hidden, out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict, edge_attr_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict, edge_attr_dict=edge_attr_dict)\n",
    "        x = self.bn1(x_dict['substation'])\n",
    "        x = torch.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x_dict = {'substation': x}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict, edge_attr_dict=edge_attr_dict)\n",
    "        x = self.bn2(x_dict['substation'])\n",
    "        x = torch.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return self.lin(x)\n",
    "\n",
    "model = HeteroGATv2Edge(metadata=g.metadata()).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.0015, weight_decay=2e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', patience=10, factor=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | loss 0.0000 | val F1 0.0000\n",
      "Epoch 010 | loss 0.0000 | val F1 0.0000\n",
      "Epoch 020 | loss 0.0000 | val F1 0.0000\n",
      "Epoch 030 | loss 0.0000 | val F1 0.0000\n",
      "\\nBest val F1: 0.0000 @ epoch 1 | Test F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train + early stopping on macro-F1 (val)\n",
    "def train_step():\n",
    "    model.train()\n",
    "    logits = model(x_dict, edge_index_dict, edge_attr_dict)\n",
    "    loss = focal_ce_loss(logits[train_mask], y[train_mask], alpha=ALPHA_CB, gamma=2.0)\n",
    "    opt.zero_grad(); loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    opt.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_f1(mask):\n",
    "    model.eval()\n",
    "    logits = model(x_dict, edge_index_dict, edge_attr_dict)\n",
    "    pred = logits[mask].argmax(1).cpu().numpy()\n",
    "    true = y[mask].cpu().numpy()\n",
    "    return f1_score(true, pred, average='macro')\n",
    "\n",
    "best_f1, best_state, patience, best_epoch = -1, None, 0, 0\n",
    "for epoch in range(1, 201):\n",
    "    loss = train_step()\n",
    "    val_f1 = eval_f1(val_mask)\n",
    "    scheduler.step(val_f1)\n",
    "\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1, best_state, patience, best_epoch = val_f1, {k:v.cpu() for k,v in model.state_dict().items()}, 0, epoch\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | loss {loss:.4f} | val F1 {val_f1:.4f}\")\n",
    "    if patience >= 30:\n",
    "        break\n",
    "\n",
    "# Final test\n",
    "model.load_state_dict({k:v.to(device) for k,v in best_state.items()})\n",
    "@torch.no_grad()\n",
    "def logits_all():\n",
    "    model.eval()\n",
    "    return model(x_dict, edge_index_dict, edge_attr_dict)\n",
    "\n",
    "log = logits_all()\n",
    "pred_test = log[test_mask].argmax(1).cpu().numpy()\n",
    "true_test = y[test_mask].cpu().numpy()\n",
    "test_f1 = f1_score(true_test, pred_test, average='macro')\n",
    "print(f\"\\\\nBest val F1: {best_f1:.4f} @ epoch {best_epoch} | Test F1: {test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reports + optional threshold tuning for binary\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "pred_val = log[val_mask].argmax(1).cpu().numpy()\n",
    "true_val = y[val_mask].cpu().numpy()\n",
    "\n",
    "print('== Val (argmax) ==')\n",
    "print(classification_report(true_val, pred_val, digits=4))\n",
    "print(confusion_matrix(true_val, pred_val))\n",
    "\n",
    "print('\\\\n== Test (argmax) ==')\n",
    "print(classification_report(true_test, pred_test, digits=4))\n",
    "print(confusion_matrix(true_test, pred_test))\n",
    "\n",
    "# Optional: threshold on validation to maximize macro-F1\n",
    "probs = torch.softmax(log, dim=1)[:,1].cpu().numpy()\n",
    "val_mask_np = val_mask.cpu().numpy(); test_mask_np = test_mask.cpu().numpy()\n",
    "\n",
    "best_t, best_val_f1 = 0.5, -1.0\n",
    "for t in np.linspace(0.2, 0.8, 25):\n",
    "    pv = (probs[val_mask_np] >= t).astype(int)\n",
    "    f1v = f1_score(true_val, pv, average='macro')\n",
    "    if f1v > best_val_f1:\n",
    "        best_val_f1, best_t = f1v, t\n",
    "\n",
    "pt = (probs[test_mask_np] >= best_t).astype(int)\n",
    "f1t = f1_score(true_test, pt, average='macro')\n",
    "print(f\"\\\\nBest val F1 via thresholding: {best_val_f1:.4f} at t={best_t:.3f}\")\n",
    "print(f'Test F1 at tuned threshold:   {f1t:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMGNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
